[
  
  {
    "title": "Semaphore - An open-source Ansible GUI",
    "url": "/sysblob.github.io/posts/semaphore/",
    "categories": "homelabbing",
    "tags": "ansible, semaphore, automation",
    "date": "2023-09-09 00:00:00 -0400",
    





    
    "snippet": "Semaphore is an open-source browser based GUI for Ansible. Semaphore has a slick interface and executes playbooks by reading them directly from a Github repository you give it credentials to. I wou...",
    "content": "Semaphore is an open-source browser based GUI for Ansible. Semaphore has a slick interface and executes playbooks by reading them directly from a Github repository you give it credentials to. I wouldn’t necessarily say Semaphore is a complete package for Ansible but for the purposes of a homelabber it will work great. In fact Ansible really only has two main popular GUI associated with it, Ansible Tower or AWX, and Semaphore. Ansible Tower uses kubernetes and can get very complicated. If you want a simple open-source option, weirdly, I’ve really only seen Semaphore.Before we jump into Semaphore it’s important that you have an understanding of Ansible as a whole. If you need to brush up on any topics I’ve also written a deep dive on it found here: Ansible the automation kingSemaphore OverviewSemaphore runs in a Docker container to create a smooth browser based Ansible experience. Semaphore includes a place to edit and store your inventory files, an environment for variables, keystores for your SSH connections, and of course an interface to run your playbooks. As you can see below editing your inventory file is right there in the interface and easy to use.One major gripe I have about Semaphore is it doesn’t let you edit your Playbooks within the GUI. I’m honestly not sure what the limitation here was and it seems a little silly. Instead, Semaphore runs all your playbooks directly from a Github repository. This isn’t the end of the world as versioning and managed playbooks via Github is a nice bonus. There are options to edit in the playbook GUI but it’s mostly just specifying environment and inventory type connections.Semaphore SetupSemaphore as mentioned previous installs via a docker container and is pretty easy to setup. Run this docker compose file after editing.If you need assistance with Docker see my Docker guide: Docker indepth diveservices:  mysql:    restart: unless-stopped    ports:      - 3306:3306    image: mysql:8.0    hostname: mysql    volumes:      - semaphore-mysql:/var/lib/mysql    environment:      MYSQL_RANDOM_ROOT_PASSWORD: 'yes'      MYSQL_DATABASE: semaphore      MYSQL_USER: semaphore      MYSQL_PASSWORD: semaphore  semaphore:    restart: unless-stopped    ports:      - 3000:3000    image: semaphoreui/semaphore:latest    environment:      SEMAPHORE_DB_USER: semaphore      SEMAPHORE_DB_PASS: semaphore      SEMAPHORE_DB_HOST: mysql # for postgres, change to: postgres      SEMAPHORE_DB_PORT: 3306 # change to 5432 for postgres      SEMAPHORE_DB_DIALECT: mysql      SEMAPHORE_DB: semaphore      SEMAPHORE_PLAYBOOK_PATH: /tmp/semaphore/      SEMAPHORE_ADMIN_PASSWORD: passwordhere      SEMAPHORE_ADMIN_NAME: lucyadmin      SEMAPHORE_ADMIN_EMAIL: emailhere      SEMAPHORE_ADMIN: lucyadmin      SEMAPHORE_ACCESS_KEY_ENCRYPTION: gs72mPntFATGJs9qK0pQ0rKtfid6MHy9bH9gWKhTU= # long string which authenticates clients      SEMAPHORE_LDAP_ACTIVATED: 'no' # if you wish to use ldap, set to: 'yes'     depends_on:      - mysql # for postgres, change to: postgresvolumes:  semaphore-mysql: # to use postgres, switch to: semaphore-postgresSemaphore should now be available on port 3000 via hostname:3000 in your browser.Using SemaphoreThere are a few quirks you need to do to get Semaphore executing playbooks. I have made note of them to make your life more simple.First define an environment. If you don’t know what this is keep it simple and make your environment like mine.Host key checking is a variable I like to have disabled. This is what controls checking host connections against your known_hosts file. You can leave this turned to the default of on but you then need to maintain a good known_hosts file on the semaphore linux host to all the hosts you intend to connect to via SSH. Since we’re internal and safe here anyway, I just disable it.We also need to set our SSH keys for Ansible or specify username and password. Semaphore allows you to create both methods here. Also while you’re here you can create the password variable used for when you do sudo commands (or passwords for become_user).Finally, don’t forget to create an inventory file and specify your Github repository. The URL should be the direct URL of your repository so in the format of https://github.com/username/semaphore_repo is what it’s looking for. You can then create any directories you want in your github to organize your playbooks. Here is an example of mine.When you add your playbook you can then add the path directly to your playbook filename line.SummaryI knew very early Ansible is one of those tools that just works better inside a GUI. Frankly, I’m shocked there aren’t more alternatives to Semaphore out there. I’m very happy with what it can do for my lab though as it allows me to execute my playbooks without having to muck through memorizing or looking up commands. With Ansible semaphore I keep my VMs and containers updated, do quick reboots, and even use it for some installation and setup processes. While it doesn’t capture some of the more complex workings of Ansible it certainly gets the job done in a world where there doesn’t seem to be anyone else."
  },
  
  {
    "title": "Ansible the automation king",
    "url": "/sysblob.github.io/posts/ansible/",
    "categories": "enterprise",
    "tags": "ansible, automation",
    "date": "2023-09-08 00:00:00 -0400",
    





    
    "snippet": "Ansible, is an agentless python-based automation tool. It works with playbooks and inventory lists in order to execute state changes based on system information it pulls before execution called fac...",
    "content": "Ansible, is an agentless python-based automation tool. It works with playbooks and inventory lists in order to execute state changes based on system information it pulls before execution called facts. Ansible is a configuration management tool and can both keep your machines in a certain state, or make changes on the fly. Ansible along with chef and puppet make up a very large percentage of the enterprise market when it comes to these tools. In other words, if you’re in a large enterprise linux environment, it’s likely they’re using it. And why not? Ansible is great for large companies and homelabbers alike. The better you get with ansible the more you begin to see it’s capable of automating anything.Introduction to AnsibleAnsible uses playbooks which are yaml based files. These playbooks are meant to work with Ansible’s many modules in order to send commands to remote servers. Ansible does this via SSH, so it does not require an agent. First Ansible makes the SSH connection using your key or another method, then it pulls data on the state of the machine - we call these facts. Using this state data, Ansible then executes your playbook to make the state of the machine match the state of the playbook (Such as “all apt packages are updated”). Your inventory is a file where you store a list of the servers you want to act upon.Ansible InstallationAnsible can be setup in a PUSH or PULL configuration, but this guide focuses on the push method using an Ubuntu server. In this method a server is setup as a controller in order to carry out commands against your hosts.Prerequisites: Ubuntu Server, Python –sudo apt install software-properties-common # needed for using add-apt-repository command if not installedsudo add-apt-repository --yes --update ppa:ansible/ansible # add latest ansible repository and updatesudo apt install sshpass # for sshing using a password automationansible --version # check to make sure it worksSimple Ansible directoriesWhile Ansible can have a complicated file structure for managing roles and groups, here we will just create the necessary basic folders. Create a structure like this:Ansible/├── inventory.ini (this can be a yaml or ini file. I prefer ini)├── ansible.cfg|   ├── Playbooks/ (directory contains playbooks)│   ├── facts.yml│   ├── update_hosts.yml│   └── reboot_hosts.ymlInventory example[ubuntu_hosts]subdomain.example.comothersubdomain.example.com[ubuntu_hosts:vars]ansible_user=ansible_manansible_ssh_private_key_file=/home/ansible_man/.ssh/ansible_key.pem[rhel_hosts]rhelexample.example.com192.168.10.10[rhel_hosts:vars]ansible_user=rhel_useransible_ssh_private_key_file=/home/rhel_user/.ssh/rhel_key.pemHere we are simply specifying two host types, ubuntu_hosts and rhel_hosts. Servers can be listed in FQDN format or in IP address format. Next we define variables to be used when ansible is acting upon these servers using the server:vars format. We use this here to specify the user to login to the server with as well as the private key to use. Environment variables don’t have to be used in an inventory file though! In fact, ansible allows for specifying these types of variables within the inventory, within playbooks, one off commands, or even in an environment variables file. How you manage your variables depends on design preference and use case.Next let’s create a Playbook file.First Playbook  Keep in mind YML files are based off tabbing structure. If the tabs aren’t correct, it will not work.--- - hosts: all   become: true   become_user: root   pre_tasks:    - name: install updates (Ubuntu)     apt:       update_cache: yes       upgrade: 'yes'     when: ansible_distribution == \"Ubuntu\"    - name: install updates (RHEL)     yum:       name: '*'       state: latest     when: ansible_distribution == \"Rocky\"Let’s break down the playbook.  hosts: all: Telling the playbook to run on all hosts in the inventory file.  become: true: Telling the playbook we need to switch to another user once logged in.  become_user: root: Specifying the user to become.  pre_tasks:: Playbooks can be broken down into sections telling Ansible what order to run them in. For example pre and post.  name: install updates (Ubuntu): Define the name of a particular task  apt:: This is what’s known in Ansible as a Module. Modules are built-in code that can makes performing certain tasks easy by calling them.  update_cache: yes: This module is calling apt which is the package manager for Ubuntu. This tells it to do an apt update of repositories.  upgrade: 'yes': This tells it to do an apt upgrade of packages installed.  when: ansible_distribution == \"Ubuntu\": A when statement to tell it to only run this task if the OS fact is found to be Ubuntu.  The RHEL portion is the same but designed for DNF/YUM package manager.Let’s run this playbook against our inventory file. The following command points to our playbook and inventory files to run Ansible.ansible-playbook -K ~/ansible/playbooks/update.yml -i ~/ansible/inventoryThat’s great and all, but let’s save some time and set some defaults in ansible. We do this and much more by creating a simple ansible.cfg file in the base directory of /ansible.# ansible.cfg[defaults]inventory = /home/username/ansible/inventory # specify where to default look for inventory fileremote_user = username # default username to login withprivate_key_file = /home/username/.ssh/keyname.pem # default key to useinterpreter_python = auto_silent # default Python interpreter detection. Automatic is the\t\t\t\t\t\t\t\t # default as of latest version and I add silent to get rid                                  # of a warning you get with Rocky linux.Finally, If you’re using a passphrase with your SSH key it’s important to load this into an ssh-agent on your ansible controller or it won’t work properly. This caches your SSH passphrase so ansible can flip easily through hosts. Add as many keys as you use.eval \"$(ssh-agent -s)\" #run ssh agentssh-add ~/.ssh/yourkey.pem #add you key - fill out passphrase  One problem with SSH agents is by default they won’t kill themselves and they will continue to store your passphrase and key location. Sometimes if you’re not careful you can end up with multiple sessions of them running and not even know it. For this reason and many others Ansible typically accesses machines using a well guarded passphraseless key or by username and password authentication for a user specifically setup for Ansible in your environment.Now that we have this setup properly we can just specify the playbook to run ansible:ansible-playbook -K ~/ansible/playbooks/update.ymlScratching the surfaceWe’ve learned a lot. We know how to create a simple ansible directory including an inventory list with variables, and execute playbooks quick and efficiently using configuration files. While these are the basics of Ansible there are some huge concepts missing here. Below are some topics you may want to explore to up your ansible game.TagsThere’s a lot you can do with tags. Tags in their simplest form are added within playbooks so you can call a specific task or a group of tasks when necessary. An example of tag usage is found below:- hosts: \"*\"  become: yes  tasks:  - name: Install the servers    yum:      name:      - httpd      - memcached      state: present      tags:      - packages      - webservers    - name: Add database    yum:      name:       - mariadb-server      state: present      tags:      - database  - name: Copy over index file    copy:      src: ~/ansible/files/index.html      dest: /var/www/html/index.html      owner: ubuntu      group: ubuntu      mode: '0764'      tags:       - webservers      - movethefile    when: inventory_hostname == \"192.168.10.10\" or inventory_hostname == \"192.168.10.11\"Firstly you might notice in this example we use a when statement. You can run certain tasks in Ansible based off conditionals and that is what I did here.Also in this example we are using the three tags packages, memcached, and database. The playbook itself could be used to fully setup a web server with an apache handler, maria database back end, and copy the main webpage over to the website directory. However, the advantage of tags is we can add as many as we want and then call them accordingly. For example, if you wanted to install everything on all hosts you would just call:ansible-playbook -K ~/ansible/playbooks/playbook.ymlHowever, if you wanted to only setup for a web server with no database you could call the same playbook with tag webservers which matches everything but the database task:ansible-playbook --tags webservers -K ~/ansible/playbooks/playbook.ymlWhat if we had a custom html page for a server we were spinning up and didn’t want the file move portion but DID need the database? Very easily we add the appropriate tags in our call:ansible-playbook --tags \"packages,database\" -K ~/ansible/playbooks/playbook.ymlWe now understand the amazing usefulness of tags and the organization they bring to Ansible. This just scratches the surface and tags can be way more complicated including auto assigning them and special tags.IncludesAt a high level includes are used to break up playbooks into task lists which can be imported into other playbooks for organization and ease of use. Yet another way Ansible gives you tools to design your architecture however you’d like. Here we execute a debug task printing the message “task1” and then executing some tasks from another file called sometasks.yml, and then finally printing “task3”.- hosts: all  tasks:    - debug:        msg: task1    - name: Include task list in play      include_tasks:        file: sometasks.yaml    - debug:        msg: task3HandlersIn order to execute tasks only when certain conditions are met Ansible uses Handlers. Handlers are included at the end of a playbook and are only executed if they are called using a command called a notify. In this example below you can see during the playbook we ask the services to be restarted by notifying them. They will not execute this until the end of the playbook, and will not execute more than once regardless of how many times they are called.tasks:- name: Template configuration file  ansible.builtin.template:    src: template.j2    dest: /etc/foo.conf  notify:    - Restart apache    - Restart memcachedhandlers:  - name: Restart memcached    ansible.builtin.service:      name: memcached      state: restarted  - name: Restart apache    ansible.builtin.service:      name: apache      state: restartedRolesThink of roles as a way of packaging all the necessary files needed to perform a certain function all under one umbrella. The umbrella in this case, is a certain file structure.Ansible has a built in way to create this structure:ansible-galaxy init /path/to/role --offlineHere is a directory tree example of what Ansible creates:/etc/ansible/roles/apache/|-- README.md|-- defaults|   `-- main.yml|-- files|-- handlers|   `-- main.yml|-- meta|   `-- main.yml|-- tasks|   `-- main.yml|-- templates|-- tests|   |-- inventory|   `-- test.yml`-- vars    `-- main.ymlNow that we have a directory structure setup we can fill in the files. Tasks can be pasted into the /tasks main.yml or they can be put in their own files and called as includes within main.yml. Handlers, templates, variables, and everything we need is contained within the role. We might create a role for setting up apache web servers and these files would handle every scenario surrounding that. Now we just simply call the role within another playbook like so.--- - hosts: node2   roles:   - apacheTemplatesTemplates are typically used to edit and create configuration files dynamically. Let’s say you wanted to use Ansible to generate an HTML page which had a dynamic title and post description. It’s best to see a practical example for this by looking at both the file that is templated, and the playbook which inserts variables into the template.The template file (template.html)&lt;!doctype html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt;  &lt;meta charset=\"utf-8\"&gt;  &lt;title&gt;&lt;/title&gt;  &lt;meta name=\"description\" content=\"Created with Ansible\"&gt;&lt;/head&gt;&lt;body&gt;    &lt;h1&gt;&lt;/h1&gt;    &lt;p&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;You’ll notice this has two variables waiting to be filled in within the template, page_title and page_description.The playbook:---- hosts: all  become: yes  vars:    page_title: My Landing Page    page_description: This is my landing page description.  tasks:    - name: Install Nginx      apt:        name: nginx        state: latest    - name: Apply Page Template      template:        src: files/landing-page.html.j2        dest: /var/www/html/index.nginx-debian.html    - name: Allow all access to tcp port 80      ufw:        rule: allow        port: '80'        proto: tcpIn this playbook the two variables we wanted to fill in when we executed the playbook are defined. We then execute the playbook, the variables will be filled into the appropriate HTML variable spots, and we will have a complete file deployed.SummaryI always hesitate to post about broad topics such as Docker or Ansible because there is just a massive amount of information to cover. This is definitely the case here where I feel I cannot possibly cover all the information that is Ansible. I hope though that this was a solid glance into the Ansible world in a cohesive and well explained way and some of the key terms have been introduced.As always I like to link to source material if I feel they’re doing a great job. If you really want to learn Ansible I would take a look at the guides over at Learnlinux.tv found here: Learn Linux - Ansible Guides"
  },
  
  {
    "title": "An Overview of Linux",
    "url": "/sysblob.github.io/posts/linux/",
    "categories": "enterprise",
    "tags": "linux, rhel, terminal",
    "date": "2023-09-05 00:00:00 -0400",
    





    
    "snippet": "Everyone has seen a movie where a hacker is typing away on a black screen and code whizzes by. This is our ingrained image a computer super genius. The black screen is something called a terminal w...",
    "content": "Everyone has seen a movie where a hacker is typing away on a black screen and code whizzes by. This is our ingrained image a computer super genius. The black screen is something called a terminal which allows you to enter commands that are interpreted directly into programs. Linux is famous for being a terminal heavy operating system, and one hackers and homelabbers alike love to run. Let’s take a look at some of the concepts behind Linux.IntroductionYou could write an entire book on the amount of knowledge there is to cover on Linux - and many people have. I spent a lot of time gathering and compiling all the information you’ll find here. While not a complete guide to linux, there is a massive amount of information in this guide. As such I highly recommend you make use of the table of contents on the right hand side. let’s get linux installed for the first time if you haven’t.I’m a big fan of learnlinux.tv as a website. Jay makes absolutely great content for everything linux. Speaking of which, if you’ve never installed a linux system before, here is his guide on Ubuntu linux so you have a baseline to work with.Linux DistributionsOne of the things I wanted to cover that I found confusing when I first started using linux is the concepts of distributions and streams. Linux comes from the open source community, meaning it was created for free as an operating system. Since it was free and the code was available, many people created their own versions or forks of linux. We call these different coding forks distributions. The interesting thing is it didn’t stop there. People then forked these distributions, which made forks of forks. When you have a chain of distributions which depend on each other we call this a stream. You can refer to a linux distribution as being upstream or downstream of another distribution. Let’s look at the most famous linux stream as a concrete example.Redhat Enterprise Linux or RHEL for short, is the de facto enterprise linux distribution. If linux is free you ask, how can a distribution be enterprise? Well, support is your answer. RHEL puts features and customer support infrastructure into their product so that they can charge money for a more substantial product. Companies like to know if something breaks someone will stand by it and help them.Upstream of RHEL sits Fedora, the open source version of RHEL. The community, along with RHEL developers, actively contribute to the Fedora operating system. This code is then used and modified to become RHEL. This coding cycle continues, with one release based off another within the chain. Downstream of RHEL sits several other distributions such as Oracle Linux, Rocky Linux, and Alma Linux. The obvious question would be how can distributions like Rocky Linux take code from RHEL an enterprise version? This is exactly a problem in the linux distribution world. Another distribution, CentOS, used to be the official open-source downstream of RHEL and Rocky and Oracle were based off that. Recently RedHat made very controversial decisions to move CentOS upstream, and no longer make enterprise code available downstream. This has put distributions in a tricky spot where they want to remain 1:1 with RHEL, but struggle to gain insight into RHEL’s codebase.A list of some commonly known Linux distributions sometimes refered to as flavors:  Rocky https://rockylinux.org/: Rocky strives to be a 1:1 downstream clone of RHEL.  Alma https://almalinux.org/: ABI compatible clone of RHEL.  Oracle https://www.oracle.com/linux/: Oracle’s enhanced security Linux downstream of RHEL.  Fedora https://fedoraproject.org/: Upstream open-source of RHEL.  CentOS Stream https://www.centos.org/: Positioned midstream between Fedora and RHEL.  Debian https://www.debian.org/: Major RHEL alternative open-source stream which forms Ubuntu.  Ubuntu https://ubuntu.com/: Solid linux alternative to RHEL made by Canonical. First choice for a lot of homelabbers and downstream of debian.  Arch https://archlinux.org/: A distro based around constant rolling updates.  Kali https://www.kali.org/: A distro designed around hacking. Popular among penetration testers.Most of these distros can come with a GUI or they can come as what we refer to as headless which basically means terminal only.Navigating linuxSo you’re sitting at a command prompt. You’ll need to get the lay of the land a bit. Navigating it the first step so here is a great set of commands to begin moving around by typing them and pressing enter within your terminal.  cd : change directory. Ex. cd /var/log  pwd : lists your current directory.  ls : lists out files in directory. Can be used with ls -a for hidden files. Ex. ls -a /var/logIt’s important to note here that some characters in linux have special meanings in the terminal. See below.(.) The dot represents the current directory in the filesystem.(..) The dot-dot represents one level above the current directory.(/) The forward slash represents the “root” of the filesystem. (Every directory/file in the Linux filesystem is nested under the root / directory.)(~) The tilde represents the home directory of the currently logged in user.(*) The asterisk represents a wildcard and is basically used to extend search matches in commands. We’ll see that below a couple times.So in other words you could change to your home directory by typing cd ~ or go one directory backwards with cd ..Key directoriesLinux has some file paths that are in common amongst its distributions. It’s important to have a general sense of where things are stored so that if you’re looking for a file type you know where to start.Root directories/ETC: configuration files/HOME: individual user’s login directories/BOOT: Boot loader files/ROOT: The root directory/OPT: Third-party applications/DEV: Device files/BIN: User binaries/SBIN: System binaries/USR: User Applications/PROC: Process information/MNT: Mount directory/SYS: Virtual file system/MEDIA: Removable devices/RUN: Temporary file system/TMP: Temporary files/LIB: System libraries/SRV: Service data directoryloggingIn general logs can be found within the /var/logs directory. Important log files here generally include:/var/log/messages – Contains global system messages, including the messages that are logged during system startup./var/log/boot.log – Contains information logged when the system boots/var/log/lastlog – Displays the recent login information for all the users. Can use LASTLOG command./var/log/yum.log – Contains logs for packages installed using yum. Or /var/log/dnf.log for example.FIREWALLD and DNF, and Cron job logs are located here as well.Important commandsGeneralThe MAN command is a searchable manual of every command in linux.manex. man grep # display a page on using the command grepex. man -f ls # searching within the manual for entries concerning \"ls\"The PWD command prints the absolute path of where you’re located. This is great is you’re lost within the file system or you need to copy and paste a path somewhere.pwdex. pwd # displays current directory/home/Ubuntu/DockerThe CD command changes the current working directory.cdex. cd /home/user/docker # changes directly to /home/user/docker directoryex. cd ~ # Using tilde changes to home directoryex. cd .. # Using two periods changes to one directory upex. cd - # Using dash changes back to last working directoryThe SSH command is used to connect remotely to a server using secure shell protocol.sshex. ssh -i ~/.ssh/key.pem username@servername # -i specifies filename of SSH key and mary would be the usernameThe HISTORY command can be used in conjunction with the exclamation mark to reuse any recent series of linux commands.historyex. history # displays last commands usedex. !27 # use a command from displayed listThe LS command lists files and directories.lsex. ls -a  # This shows all files including hidden directories and system filesex. ls -sh # Shows file sizes, the h portion makes it human-readableex. ls -d */ # list only foldersex. ls -l # show ownership of a fileThe MKDIR command creates a folder.mkdirex. mkdir newfolder # mkdir followed by folder name creates a directoryex. mkdir this/series/of/directories -p # The -p command will create all parent directoriesThe MV command is equivalent of the cut command. This is also how Linux renames a file or folder.mvex. mv ~/Documents/Ukulele/Apache.pdf /etc/ # mv command works by specifying source path and destination pathThe CP command is equivalent of the copy command.cpex. cp file.a file.b /home/usr/rapid/ # copies file.a and file.b to /home/usr/rapid folderex. cp -r /source/folder/ /remote/place/ # source, destination -R command recursively copies all files in directoryex. cp *.c dest/folder # copies all files with .c extension to dest/folderThe SCP command is the secure remote transfer version of copy.scp -i .ssh/mykey -v \"somefile.txt\" ubuntu@sysblob:/home/ubuntu/The RM command can delete files, and RMDIR removes a directory. RM is often used with -r to recursively delete directories.rm somefile.exerm -rf /somedirectory # -r is recursive -f is force to ignore errors such as empty directoriesrmdir /someemptydirectory #rmdir does not work if folder contains filesThe ALIAS command can shorten a longer command or a series of commands.aliasex. alias cls=clearex. alias pf=\"ps -e | grep $1\" # Make pf command and whatever follows it search for a processSearching and pipingThese commands are for searching for files or through content, or for performing piping. In Linux it’s possible to feed the results of one command into the next command and we call that piping.The FIND command will search for folders or files.findex. find . -name *ones* -type f # Searches for files. -name specifies the sequence -type can return d(irectory) or f(ile)ex. find /home/user -name *.txtex. find Pictures/ type -f -exec chmod 600 {} + # can be used to mass edit permissions. {} holds value per line and + ends/loops lineThe GREP and PIPE ‘|’ command are extremely useful and are often used together. Grep allows for searching and pipe is how you feed a command into another. Using pipe you can chain together the results of any linux command into another linux command to filter results.grep and |ex. ip addr | grep \"inet\" # only display the ip addresses on a linux system without the junkex. grep \"string\" filename.txt # search a file for a stringex. grep -r \"192.168.1.5\" /etc/ # displays lines that contain that IP within a fileex. cat /proc/cpuinfo | grep -i 'Model' # reads a directory and only prints out lines with ModelProcesses, CPU, and MemoryThe PS and KILL commands can be used with grep to check quickly for processes and end them.psex1. ps -e | grep shutter # first search for a process     kill 1692 # then kill it by idThe TOP and HTOP commands are useful for displaying data on CPU and memory usage. HTOP is colored and real-time. You can also kill processes and run other interactive commands using the function keys.htopex. htop -u username # displays processes of specific userThe FREE command simply displays memory statistics.freeex. free -h # memory statistics in human readableStorageThe DF command can be used to check total storage. It’s best used with the human readable option.dfex. df -h # display filesystem and usage in human readableThe LSBLK command lists block devices on system. Block devices are typically SSDs or HDDs.lsblkex. lsblkThe DU command can be used to check particular directories for space usage.duEx. du -h /var/log # find out what's taking up space in /var/logContentThe TOUCH command can be used to quickly create a file without content.touchex. touch test.txtex. touch index.htmlThe CAT command is great for working with anything text. It has a multitude of uses from copying, reading, or creating content.catex. cat \"put this string in there\" &gt; example.txt # create a file and put some text in thereex. cat example.txt # display file contentsex. cat file1.txt &gt;&gt; file2.txt # append one file onto anotherex. cat file1.txt file2.txt file3.txt &gt; combinedfile.txt # combine multiple files into one big fileex. cat &gt;&gt; file.txt # type multiple lines into a fileex. tac file.txt # tac is like cat but displays content backwardsThe ECHO command is similar to CAT but is used to mostly display variables within linux.echoex. echo $USER # display the user variableex. echo \"This is the list of directories: $(ls)\" # echo can even be used in scripts to make things readable.The WGET command is used to retrieve content across a network.wgetex. wget http://www.filesite.com/downloads/files.zipThe CURL command sends a GET request to a web server. This can be used to retrieve any content the web server responds with.curlex. curl www.google.comex. curl http://www.filesite.com/downloads/files.zipThe TAR command is used for archiving a directory into a singular file called a tarball. It has many important options:  f specifies filename  c creates  x extracts  t displays files inside zip  z declares file as gzip type  v verbosetar ex. tar -cvf etc_backup.tar etc # creates tarballex. tar -tvf etc_backup.tar # view files inside tarballex. tar -xvf etc_backup.tar # extract the contents of a tarballex. tar -czvf etc_backup.tar.gz etc # use both tar and gzip on a file to compress. add .gz best practice.The GZIP command is typically used within the TAR command but is used to compress a file.gzipex. gzip etc_backup.tarex. gunzip etc_backup.tar.gz # unzipThe TAIL command displays the last 10 lines of a file. Often useful for log files. You can specify how many lines to display.tailex. tail -n 5 file.txt # display last 5 lines of a fileThe TRUNCATE command can shorten a file for you. Useful for emptying a file but not deleting it.truncateex. truncate /var/log/cloudinit.log -s 0 # -s is file sizeex. sudo find /var/log/ -type f *.log -exec truncate -s 0 {} + # clear contents of all var .log filesSu and SudoMuch like Windows makes use of Administrator groups, Linux uses Sudo. Using the SUDO command allows you to temporarily assume super user rights in order to perform an operation. The SU command allows you to login as a new user.sudo chmod 700 myfile # temporarily assume super rights to perform a single operationsu username # Assume the permission rights of sudo user moving forwardIn order to use the sudo command we need to be part of the sudoers group. There are two ways to modify this group to add rights. You can directly add the user via nano and the /etc/sudoers file, or you can use the usermod or visudo commands. The visudo command does the same thing, except designed specifically with safety in mind. For example, it uses an editor which locks the sudoers file and prevents other users from editing it while you have it open.Using USERMOD to add a user to sudo group:sudo usermod -aG sudo username # append user to sudo groupUsing VISUDO to add a user to sudo group.sudo visudo## add similar line inside filedaniel ALL=(ALL:ALL) mytop,cat,tail # you can also give a user sudo rights only for certain appsUsers, Groups and permissionsThe LASTLOG command displays the last login time for a user.lastlogex. lastlog -u username # displays last login time for a userThe GROUPS command displays current groups of a user.groupsex. groups username # displays groups for a userThe CHOWN command changes ownership of a linux file. (For more details see the Linux file permissions section).chownex. chown user file.txt # make user the new owner of fileex. chown user:group file.txt # change user and groupex. chown :group file.txt # just change groupex. chown -R user:group /that/folder/directory # recursively give ownership to directory and filesThe CHMOD command modifies file permissions for the folder or file. (For more details see the linux file permissions section).File permissionsLinux uses a permissions system that at first glance can be pretty confusing. However, simply put they can be broken down into two categories: Permission Group, and Permission type.When we create a new user, entries are made in five files - /etc/passwd /etc/shadow /etc/group /etc/login.defs and /etc/gshadow. The purposes of these files are as follows:PERMISSION GROUPu – Ownerg – Groupa – All usersPERMISSION TYPESr – Readw – Writex – Execute - It’s important to note that execute rights are required to enter/CD into a directoryTogether we use these commands to control file permissions. Let’s look at an example of an ls -l (which displays files and their permissions):-rw-rw-r-- 1 lucyadmin lucyadmin  203 Oct  6 07:03 mylogo.txtIn this example the -rw-rw-r– gives us a lot of information. The first letter ( - ) represents that this is a file. A dash represents a file or a d represents a directory. The second portion rw- represents the owner’s rights. This owner has read, write, but does not have execute rights (represented by the dash). The third portion rw- represents the permissions of the group which has read and write access. The final portion of r– represents the any user category, which only had read permissions.OCTAL NOTATION            Number      Permission      Symbol                  0      No permission      —              1      execute      –x              2      write      -w-              3      execute + write      -wx              4      read      r–              5      read + execute      r-x              6      read + write      rw-              7      read + write + execute      rwx      We then use OCTAL NOTATION in combination with CHMOD to change file and directory permissions.chmod 764 myfile # changes to permissions 7, 6, and 4 for user, group, and any respectively.Now that we have a good grasp on how Linux deals with and writes permissions it’s important to learn octal notation. There are only so many ways a file or directory can have permissions. To make things easier on ourselves we use numbers to represent certain permission combinations. We always know the order will be USER:GROUP:ANY with a possible 7 combinations so we represent permissions by a 3 digit number with each digit being 1-7. The chart below explains:PASSWD - Globally readable file that contains username and encrypted password information.SHADOW - Admin only file which contains sensitive password and login information to prevent copying PASSWD file for brute force hacking.GROUP - This file defines what users belong to what groups.GSHADOW - Admin only shadow file for groups.LOGIN.DEFS - Shadow password suite configurationWe could modify these files with nano to manually add users, but in order to quickly perform these operations Linux provides built in commands.The USERADD command is used to create a new user which modifies all these files instantly. By default, in most Linux distributions creating a new user will add them to a group of the same name as their username.sudo useradd usernameex. sudo useradd -d /var/mike -m mike # -d option specifies a home directory otherwise it defaults to /home/username. -m creates a home directory.ex. sudo useradd -e 2020-05-30 test_user # add expire date# can also use -u and -g to specify own userid or groupidex. sudo useradd -r systemuser # use -r to create system userThe USERDEL command deletes a user.userdelex. sudo userdel -r username # -r deletes home directoryThe PASSWD command sets a password for a user.passwdex. sudo passwd usernameex. sudo passwd -l username # can also be used to lock a user's account. -u to unlock.The CHAGE command can be used to deal with account and password expirations.chageex. sudo chage -l username # lists expiration info for accountex. sudo chage -E 2022-12-10 username # set account expirationex. sudo chage -M 30 username # set password expiration in days. set to -1 for never expireThe GPASSWD command is used to perform admin tasks on groups. You can add or remove users from secondary groups for example.sudo gpasswd –d username groupname # remove a user from a groupsudo gpasswd -a groupname # add a user to a groupThe GROUPADD command is used to create a new group.sudo groupadd mynewgroupThe GROUPS command displays current groups of a user.groups username # displays groups for a userThe USERMOD command lets you modify an existing user data. USERMOD, for example, is the preferred way of modifying primary group membership.sudo usermod -a -G alpha username # adds username to alpha primary groupsudo usermod -l newname username # rename a username newnameThe CHMOD command lets you modify file permissions.chmodex. sudo chmod 700 .ssh # allow read, write, and execute only for userex. sudo chmod 600 .ssh/authorized_keys # allow read and write for user only ex. sudo chmod u=rw,og=r new_file.txt # chmod also allows for symbolic notation. ex. sudo chmod g-x file.txt # take away execute rights from groupex. sudo chmod -R directory/otherdirectory # recursively change permissions with -RThe GPASSWD command adds a user to a group. -d command removes a group.sudo gpasswd -a username groupnameThe CAT /ETC/PASSWORD commands are best used to list users for a server. This can also be used with grep. This is displayed in the format of: username:x:userid:groupid: , , , :/home/username:/bin/bash – each portion defining a different property of the user and the last two being the user’s home directory and default shell.cat /etc/passwdex. cat /etc/passwd | grep \"bob\"Primary and secondary groupsEvery user belongs to both a primary and secondary group. Simply put, primary groups define what group assigned ownership for newly created files, while secondary groups allow a user additional permission. We represent these group types by denoting either -g for primary or -G for secondary.sudo useradd -g primarygroup -G secondarygroup1,secondarygroup2,secondarygroup3 usernameUID and GIDYou can determine a user’s GID and UID by using the ID command. You can also CAT /ETC/GROUPS.id usernamecat /etc/groupsUSERMOD and GROUPMOD commands can be used to edit UID and GID respectively. The -n option can rename. GROUPDEL can be used to delete a group.usermod -u 2005 username # changing a users UIDgroupmod -g 3000 groupname # changing a users GIDgroupdel groupname # delete a groupWhen a user’s UID or GID are changed the ownership of home directory files are changed automatically. However, files outside this directory need to be changed manually.Expiring or UnlockingYou can instantly expire a user’s password with the -e option. The next time the user logs in they will need to reset their password.sudo passwd -e username # Expire a passwordsudo passwd -u username # Unlock an accountManaging SSHSecure Shell, or SSH, is a secure protocol used to connect to devices across unsecure networks. SSH makes use of public and private keys in order to secure both the client and server-side connection. In order to make use of SSH you first need to generate a public and private key to use.Generating an SSH keyFor generating SSH keys we prefer the ED25519 format as it’s the most secure.Use the SSH-KEYGEN command to generate public and private SSH keys.ssh-keygen -t ed25519 -C \"key comment\"If your host already has SSH keys, you may be asked if you wish to overwrite your previous key. If you choose yes, your previous key will be overwritten, and you will no longer be able to log in to servers using that key. Because of this, be sure to overwrite keys with caution..PEM vs .PPK vs .PUB.pub file format is used by SSH for public key store, this is the key you need to share..pem(Privacy Enhanced Mail) is a base64 container format for encoding keys and certificates. A PEM file can contain any string of text specified in its header. Often found in openssh format..ppk(Putty Private Key) is a windows ssh client, it does not support .pem format so private keys need to be converted to ppk.Disabling password AuthIf you have SSH keys configured, tested, and working properly, it’s likely a good idea to disable password authentication. This will prevent any user from signing in with SSH using a password.To do this, connect to your remote server and open the /etc/ssh/sshd_config file with root or sudo privileges:sudo nano /etc/ssh/sshd_configInside of the file, search for the PasswordAuthentication directive. If it is commented out, uncomment it. Set it to no to disable password logins.After you have made the change, save and close the file. To implement the changes, you should restart the SSH service.sudo service ssh restart # ubuntusudo systemctl restart ssh # rhel basedManaging Linux SSH via .ssh/config fileIf you’re remoting from a Linux based OS, Linux allows for a creation of a config file to make the process easier. Go into the .ssh folder and create a file called config. Use nano/vim to edit the file and the format per host looks like:Host nextcloudHostName 192.168.1.20User vegaIdentityFile ~/.ssh/mykey.pemThis will allow you to then use the ssh command to an alias in the form of:ssh nextcloudThe .ssh/config file is particularly important when using in combination with Github.com and private repositories. In order to pull a repository remotely via SSH github needs to know which key to use. We specify this in the host file as such:Host github.comIdentityFile ~/.ssh/private_key.pemNetworkingThe IFCONFIG and IP commands can both be used to display network interfaces. The IP command is the newer preferred command.ifconfig # display network interface detailsifconfig eth0ifconfig | grep \"inet\" # display ipv4/ipv6 detailsip addr # display network interface detailsip addr show dev em1 # show one deviceip route # display and edit route table entriesip route add default via 192.168.1.1 dev em1 # add a route to tableip addr add 192.168.1.1/24 dev em1 # add an ipOne of the commands I tend to use to list any addresses the host knows itself as is:ip addr | grep \"inet\"Firewalld and IptablesThe default firewall of RHEL based distributions is IPTABLES, however we wrap that in FIREWALLD for ease of use.  Through this firewall you can manage which ports are accessible on the server.firewall-cmd --list-all # check firewall status### FIREWALLD has a runtime config and a permanent config. Permanent config is loaded at each boot and replaces runtime. You can make permanent changes to the firewall by either editing the permanent config and reloading the firewall, or setting new runtime configs and making them permanent.# runtime to permsudo firewall-cmd &lt;options&gt;sudo firewall-cmd --runtime-to-permanent# perm and reloadsudo firewall-cmd --permanent &lt;options&gt;sudo firewall-cmd --reload### add/remove a portfirewall-cmd --permanent --add-port=22/TCPfirewall-cmd --permanent --remove-port=444/tcp### add/remove a servicefirewall-cmd --permanent --add-service=sshfirewall-cmd --permanent --remove-service=mysql### add/remove ip or rangefirewall-cmd --permanent --add-source=192.168.1.9 or /24firewall-cmd --permanent --remove-source=192.168.1.100Linux’s legacy firewall IPTABLES can be found below:iptables -nvL # list firewall rulesiptables -A INPUT -s 192.168.1.1 -j ACCEPT # white list an ip addressiptables -A INPUT -s 192.168.0.1 -j DROP # block ip addressiptables -I INPUT -p tcp --dport 25 -j ACCEPT # add port inboundiptables -I OUTPUT -p tcp --sport 25 -j ACCEPT # add port outboundiptables -A INPUT  -p tcp -m multiport --dports 80,443 -j ACCEPT # grant access to multiple ports inboundiptables -A OUTPUT -p tcp -m multiport --sports 80,443 -j ACCEPT # grant access to multiple ports outboundiptables -t nat -A PREROUTING -i eth0 -p tcp --dport 80 -j REDIRECT --to-port 443 # setup port forwardingiptables-save # save changesChecking portsTwo popular commands for checking open ports are NMAP and NETSTAT.sudo nmap -sT -O localhost # show ports in nmap locallysudo netstat -tulpn | grep LISTEN # show listening ports in netstat locallySometimes it’s useful to know when a port on a remote server is open. We can also scan for open ports remotely using using NMAP:sudo nmap 192.168.0.1 -p 22 # checking ip to see if port 22 is openNetwork filesOften network files need to be modified for reasons such as changing the hostname. The four main network files are described by Redhat as:  /etc/hosts: The main purpose of this file is to resolve host names. You specify hostname and IP address much like an ARP table. It can also be used to resolve host names on small networks with no DNS server. Regardless of the type of network the computer is on, this file should contain a line specifying the IP address of the loopback device (127.0.0.1) as localhost.localdomain.  /etc/resolv.conf: This file specifies the IP addresses of DNS servers and the search domain. Unless configured to do otherwise, the network initialization scripts populate this file.  /etc/sysconfig/network: This file specifies routing and host information for all network interfaces. It is used to contain directives which are to have global effect and not to be interface specific.  /etc/hostname: This stores the name of your host.Changing a hostnameYou can change a hostname in RHEL Linux using the HOSTNAMECTL command.hostnamectl # display host statisticshostnamectl set-hostname hostname.net # set a new hostnameAfterwards we also add the new hostname to the /etc/hosts file in a format similar to this.# /etc/hosts# other stuff here192.168.1.55 hostname.domain.com hostnameSo as a checklist you should –  Edit the name of the VM in hypervisor  Log into host and run command hostnamectl set-hostname example.domain.com  Edit the file /etc/hosts to add your IP and FQDN and hostname as a line and edit anywhere it lists old name  Renew the lease with DNS server to replace hostname on network using sudo dhclient command.After finishing these tasks you will commonly run into two issues. Any hosts that previously logged into this host via SSH will need the old host fingerprint entry deleted from .ssh/authorized_keys file. Any hosts that contacted this host previously will have an entry in their DNS cache for the old hostname pointing to this IP. You will need to clear the DNS cache for it to memorize new hostname.TroubleshootingWhen checking the network path between two devices the TRACEROUTE command can be useful as it shows each hop.traceroute google.comtraceroute -n google.com # shows network delaysWhen dealing with DNS issues the DIG and NSLOOKUP commands can display DNS results.dig google.comdig google.com ANY # query servers in /etc/resolv.confnslookup google.comnslookup 192.168.10.10 # retrieve a hostname from an ipWhen needing to clear the cache of looked up DNS on a serversudo killall -HUP mDNSResponder; echo \"dns cleared successfully\"Package ManagersMuch like windows installs software Linux installs packages. For managing packages within a Linux, we can divide Linux distributions between two software managers: RPM and DKPG. These package managers both use their own popular utility to access these package managers. For RPM, YUM is typically used. For DKPG, APT is used. APT is associated with Debian based Linux distributions such as Ubuntu. YUM is used by Redhat distributions such as Fedora and Rocky.  Technically, YUM is now deprecated and the new preferred utility is DNF and will be covered briefly at the end of this guide.Yum commandsyum update package_name --security # update a package, --security option can just install security updatesyum upgrade # update packages but removes old unneeded packagesyum list package_name # search for a packageyum list available # show available packagesyum list installed # show installed packagesyum list all # show all packagesyum repolist # show repos availableyum install package_name -y # install a package, -y agrees automaticallyyum remove package_name # remove a packageyum search package_name # search for a packageDNF commandssudo dnf repolist all # # list all repos# list all packages installed -- or list all possible packages (best to grep)dnf list installeddnf lsit | grep \"package\"# install/remove packagesudo dnf install &lt;package&gt;sudo dnf remove &lt;package&gt;# check for updates to all enabled repos then upgrade themsudo dnf check-updatesudo dnf upgrade --refresh# Searchingdnf search searchtermAPT commandssudo apt update &amp;&amp; sudo apt upgrade -y # typical command structure for updating all packagesapt update  # update package indexapt upgrade # update packages to latest versionsapt search package_name # search for a packageapt list --installed # show installed packagesapt list # show all packagesyum repolist # show repos availableapt install package_name -y # install a package, -y agrees automaticallyapt remove package_name # remove a packageapt autoremove # removes unused packagesapt show package_name # show information about a packageFile systemsLinux filesystems can be very confusing to manage. As an introductory it might be helpful to run through the steps of adding a new drive while explaining some concepts and commands.If you added a new drive determine its name by using lsblk command.  First, we need to partition a drive. We logically divide up a hard drive using partitions for organization and isolation.fdisk /dev/sdalsblk # after using fdisk we can check drive capacity and partitions using lsblkA word on drives:  SD represents SSD or SCSI device.  HD represents IDE hard driveWhen you see a storage device within linux it will be referenced this way such as /dev/sda. These drives are incremented by letter as they are discovered by the operating system, and by number for partition. For example, if an SSD drive were detected second, and we created 1 partition on it as the primary, this would be referenced by /dev/sdb1.  Next, we can create a file system on our partition with the MKFS command.mkfs.ext4 /dev/sda1We now create a mount point and mount the file system.mkdir /newstorage # create directorymount /dev/sdb1 /newstorage # mount storage at directory pointdu -h /newstorage # confirm storage is accessible and right sizeFSTAB stands for file system table and defines the way file systems are treated when they are introduced into the operating system. In other words, fstab controls how drives are automatically mounted. It’s important if you wish to mount a file system at boot to add an entry for it in /etc/fstabThis is an example of an fstab with a couple entries:# &lt;file system&gt;     &lt;dir&gt;       &lt;type&gt;   &lt;options&gt;   &lt;dump&gt;\t&lt;pass&gt;10.10.0.10:/backups /var/backups  nfs      defaults    0       0# automount from synology box192.168.1.120:/volume2/Media /mnt/media nfs nouser,rsize=8192,wsize=8192,atime,auto,rw,dev,exec,suid 0 0# as another example below192.168.1.10:/volume1/sharedfolder /localfolder nfs defaults 0 0You can mount all fstab entries live by using the mount -a command.We can check to see if the drive is successfully mounted using the df -h command.mkfs.ext4 /dev/sda1Introducing LVMAnother way of designing storage is through volumes. LVM can be thought of as bringing virtualization concepts to storage. Instead of seeing many physical devices, we combine them all into a single virtual device which makes dividing up resources easy and efficient. Let’s run through the same process but this time combine our storage using LVM or Logical Volume Manager.First, we create a PHYSICAL VOLUME. Physical volumes are disks or partitions that you want to make available to LVM as possible storage.sudo pvcreate /dev/sdb1 # here we create a physical volume out of the primary partition on our 2nd SSDWe can check our physical volumes with the PVDISPLAY command.pvdisplayNow we can create a VOLUME GROUP using the VGCREATE command. Volume groups are the virtual pool we use that contains all the physical volumes. A volume group represents usually the totality of our storage, but often can be used to divide or organize directories. By default, linux machines come with vg00 which is the first volume group. You would most likely be creating vg01.vgcreate vg00 /dev/sdb1 /dev/sdc # create volume group vg00 and add sdb1 and sdc physical volumes to itTo display Volume group information, we can use the VGDISPLAY command.vgdisplayvgdisplay vg00 # display specific groupWe then divide the volume group up into any number of LOGICAL VOLUMES. Logical volumes can be thought of as partitions of the virtual storage pool. We create a logical volume using the LVCREATE command. We don’t have to, but it’s best practice to name the logical volume the same or similar to the directory you intend to mount it to. Here I would be mounting to /home/sales.lvcreate -L 10G -n home_sales vg00 # create a 10 GB Logical Volume named home_sales carved from the vg00 Volume GroupWe can now view our newly created logical volume with the LVDISPLAY command.lvdisplay /dev/vg00/sales-lvBefore mounting we then need to assign a file system to the logical volume and format it. This is done using the MKFS command. All links to logical volumes are stored within the /dev/mapper folder which we check to make sure we are pointing at the correct volume. We then format the file system.ls -l /dev/mapper # Confirm your logical volume is in the mapper folder which is where all volumes are stored.mkfs.ext4 /dev/mapper/vg00-home_sales # format our logical volume's filesystem to ext4Once the logical volume is created it can be mounted like any other file system using the mount command. However, don’t forget to add the mount point to fstab for auto-mounting otherwise next reboot it won’t be there. We can do both of these easily by doing some echo commands into fstab and running a mount command.echo \"/dev/mapper/vg00-home_sales   /home/sales                  ext4    defaults        1 2\"  &gt;&gt; /etc/fstabThen run the mount -a command which mounts everything in fstab. We do it this way as validation as it will produce an error message if our echo to fstab was done incorrectly.mount -aExpanding a logical volumeThe general steps for adding a storage device to a LV are:  Add a disk and configure it as a physical volume.  Add it to a volume group.  Add the capacity to the logical volume and then extend the filesystem.Add the new physical volume to the volume group using VGEXTEND commandvgextend vg00 /dev/sdb2Grow the size of the specific logical volume by extending it with the LVEXTEND command.lvextend -r -L +1G /dev/vg00/sales-lv # grows size by 1 GBlvextend -r -l +100%FREE ubuntu-vg/ubuntu-lv # alternatively you can use this command to claim all possible spaceLet’s bring it all together and see a full example of creating a new physical volume, adding the physical volume to an existing volume group, allocating space for and creating two new logical volumes, and then mounting them to new directories:Cron jobsCron is a misspelling on the part of a developer, but was intended to represent the greek work Chronos or Chronological. A Cron job is a command or series of commands that are meant to run in the future. This can be a one-off command or a repeated service.Cron jobs are handled under Linux with the command CRONTAB.Crontab commandscrontabex. sudo crontab -l # list cronjobs for logged in userex. sudo cronjob -e # enters crontab editorCron jobs are controlled with crontab and editing it using the crontab -e command. Much like the /etc/sudoers file and visudo command, the crontab -e command is purposely designed for editing the /var/spool/cron/crontabs file to avoid mistakes and corruption. After opening this file via the crontab -e command, we can add a cron job as a single line at the end of the file.~* * * * * apt install tmux -y# m h dom mon dowThe first thing you’ll notice is the asterisks. Essentially the crontab file handles each entry by a space and number representing a different control for the job. These are all numerical and they can be found below:            Syntax      Description                  Asterisk #      Represents              1      minute              2      hour              3      day of month              4      month              5      day of week              6      command      So for example if we wanted a job to run on the 15th of the month at 11am to update and upgrade all packages we would simply edit in a line similar to this:0 11 15 * * sudo apt update &amp;&amp; sudo apt upgrade -yCommonly though we will call a script. This is how you’d call a bash script.0 11 15 * * /usr/bin/bash /home/username/script.shSystem users and cron jobsTypically, we don’t run Cron jobs as our day to day user. We usually set these tasks aside for a specific user created to run the job. These are system users. System users are denoted usually by a UID below 1000. We can create a system user and assign it a cron job as shown below:sudo adduser -r -s /bin/nologin apt_runner # -r is system user and -s specifies shell which we say nonesudo crontab -u apt_runner -e # specify the new user and edit their crontab documentEasy crontabsIf you find the process of writing a crontab difficult with the asterisks and timing, there are websites which help you generate the line based off easy to use templates. The website below can provide that:crontab-generator.orgJournalctlJournalctl is used to log all things controlled by systemd. It’s extremely handy to check logs to find out what went wrong. Here are some common commands that can be used.The basic command is journalctl but this dumps all possible logs from all possible time ranges. Instead, we have many ways to sort, filter, and manipulate this log data.Sorting by timejournalctl --list-boots #list logs by bootjournalctl -b #display current boot logjournalctl -b -1 #display boot log before it# other ways to sort by timejournalctl --since \"1 hour ago\" journalctl --since \"2 days ago\"journalctl --since \"2015-06-26 23:15:00\" --until \"2015-06-26 23:20:00\"journalctl -n 50 --since \"1 hour ago\" #last 50 messages last hourSorting by servicejournalctl -u nginx.service #sort by servicejournalctl -u nginx.service -u mysql.service #multipleLive loggingjournalctl -f #print log messages realtimejournalctl -u mysql.service -f #for specific serviceBash scriptingAutomation is one of the keys of linux administration and one of the basic ways to automate a series of commands is a bash script. Here are the basics of making and using a bash script.Create a script using whatever text editor. For bash scripts we use the .sh extension.sudo nano script.shsudo vi script.shPaste in#!/usr/bin/env bash## Author : Sysblob# Date: March 2022# Version 1.0.0: Displays the text \"Hello world!\"## Displays a text on the screen :echo \"Hello world!\"We can then run this bash script by using the bash command.bash script.shWe can also give the script proper permissions to be executed and run it directly.chmod 775 script.shThen run it../script.shScripts require a full path to execute unless they are added to your path which is why we specify ./ in front. We could also specify /testdir/script.sh to run it directly.  Remember in order to execute scripts you need to give the script execute permissions. Try the command chmod 775 myscript.shThe line #!/usr/bin/env bash identifies the binary used to interpret the code. You could also specify the interpreter to be python for example using #!/usr/bin/env python.VariablesHere is a good script to study to get an idea of how variables work.#!/usr/bin/env bash## Author : Sysblob# Date: July 2023# Version 1.0.0: Save in /root the files passwd, shadow, group, and gshadow# Version 1.0.1: Adding what we learned about variables## Global variablesFILE1=/etc/passwdFILE2=/etc/shadowFILE3=/etc/groupFILE4=/etc/gshadow# Destination folderDESTINATION=/root## Readonly variablesreadonly FILE1 FILE2 FILE3 FILE4 DESTINATION# A folder name with the day's numberdir=\"backup-$(date +%j)\"# Clear the screenclear# Launch the backupecho \"****************************************************************\"echo \"     Backup Script - Backup on ${HOSTNAME}                      \"echo \"****************************************************************\"echo \"The backup will be made in the folder ${dir}.\"echo \"Creating the directory...\"mkdir -p ${DESTINATION}/${dir}echo \"Starting the backup of ${FILE1}, ${FILE2}, ${FILE3}, ${FILE4} to ${DESTINATION}/${dir}:\"cp ${FILE1} ${FILE2} ${FILE3} ${FILE4} ${DESTINATION}/${dir}echo \"Backup ended!\"# The backup is noted in the system event log:logger \"Backup of system files by ${USER} on ${HOSTNAME} in the folder ${DESTINATION}/${dir}.\"System variablesSome variables come built-in to use:            Variable      Description                  HOSTNAME      Host name of the machine              PATH      Path to find the commands              PWD      Current directory, updated each time the cd command is executed              HOME      Login directory of user              $$      Process id of the script execution              $?      Return code of the last command executed              USER, USERNAME and LOGNAME      Name of the user connected to the session.      We can print environment variables using the printenv command or all variables using the set command.printenv # just global system variablesprintenv HOME # Specific variablesset # display all possible variablesThere are also Shell variables which exist inside the current shell session only. Some shell variable commands.MY_VAR=\"text here\" # Setting a shell variableecho $MY_VAR # Print variableexport MY_VAR # Mae shell variable into a permanent global environment variableexport MY_NEW_VAR=\"My New Var\" # Create a shell variable and instantly make it a global environment variablePermanent variablesPrevious mentioned variables are only temporary to your session. To make permanent variables you need to edit some files.Variables set in the file /etc/environment whenever a bash login shell is entered.FOO=barVAR_TEST=\"Test Var\"/etc/profile Use this file to set up system-wide environment variables.export JAVA_HOME=\"/path/to/java/home\"export PATH=$PATH:$JAVA_HOME/binIf you add new variables make sure to reload them into your current sessionsource ~/.bashrcStoring commands as variablesIt’s possible to put commands in variables. The syntax is:variable=`command`variable=$(command) # Preferred syntaxExamples below.$ day=`date +%d`$ homedir=$(pwd)"
  },
  
  {
    "title": "Customizing your image with Cloud-init",
    "url": "/sysblob.github.io/posts/cloud-init/",
    "categories": "homelabbing",
    "tags": "cloud-init, image editing, virt-edit",
    "date": "2023-09-05 00:00:00 -0400",
    





    
    "snippet": "One of the immediate burning questions I had when I began spinning up virtual machines was how can I automate more of the manual installation of the operating system? Clicking the same mundane opti...",
    "content": "One of the immediate burning questions I had when I began spinning up virtual machines was how can I automate more of the manual installation of the operating system? Clicking the same mundane options for every new VM is boring. I’d heard about cloud-init and seen it working in the background on some of my AWS EC2 instances or on my Ubuntu servers but couldn’t quite put all the pieces together. What is cloud-init? What are cloud-ready images? Eventually, when I got into enterprise automation, spinning up new VMs fast and efficiently became a necessity. This forced me to sit down and learn quite a bit so I could build my perfect image. Let’s take a look at cloud-init.What is cloud-init?Cloud-init is a distribution neutral way of deploying a new operating system pre-configured so it can be deployed non-interactively such as through a hypervisor template. Most distributions of linux provide “cloud ready” images which are intended to be used for this purpose. Think of cloud-init as performing the basic things you need to do before you hand off to your configuration management tool. You can do as much or as little as you need to. This can include setting up login/SSH, usernames, time zones, packages, uploading files, and much more.I’m a big fan of Rocky linux since its goal is to remain a 1:1 clone of RedHat Enterprise Linux. This makes it a great image of choice if you’re spinning up servers you’re learning on for future employment. Let’s take a look at editing the Rocky linux default cloud image.Editing an image with virt-editFirst we will download a Rocky cloud image for Rocky 8.8.https://download.rockylinux.org/pub/rocky/8.8/images/x86_64/Rocky-8-GenericCloud-Base.latest.x86_64.qcow2Alternatively download directly to your server.wget https://download.rockylinux.org/pub/rocky/8.8/images/x86_64/Rocky-8-GenericCloud-Base.latest.x86_64.qcow2This comes in the form of a qcow2 file which is a generic virtual disk. We can edit this type of file by temporarily mounting directories inside it to a host using a program called virt-edit. We install virt-edit on a generic linux machine which downloaded the image.sudo yum -y install libguestfs-tools # or aptWe set our variable for guestfs, install nano editor and assign it as default editor. Then display help.export LIBGUESTFS_BACKEND=directsudo yum install nano -yexport EDITOR=nanovirt-customize --helpAt this point you can edit files using the virt-edit commands found below. Or if you’re savvy you can edit the file which controls all cloud-init which is located at /etc/cloud/cloud.cfgvirt-edit -a Rocky-8-GenericCloud-Base.latest.x86_64.qcow2 /etc/cloud/cloud.cfgTips for editing cloud.cfg directly:  You can change disable_root to 0 in order to be able to login as root  lock_passwd can be changed to false to allow password based authentication  We can also install packages by adding:packages: - qemu-guest-agent - nano - wget - curl - net-tools  Make sure to edit the /etc/ssh/sshd_config file to allow password authentication as well if desired. This can be done using the same virt-edit command above.    PermitRootLogin yesPasswordAuthentication yes      Alternate virt-customize commandsVirt-customize allows you to inject things into your image without having to open and edit the necessary files.Setting root passwordvirt-customize -a Rocky-8-GenericCloud-Base.latest.x86_64.qcow2 --root-passwordvirt-customize -a Rocky-8-GenericCloud-Base.latest.x86_64.qcow2 --install [vim,bash-completion,wget,curl,telnet,unzip]Upload files. (local:remote)virt-customize -a Rocky-8-GenericCloud-Base.latest.x86_64.qcow2 --upload rhsm.conf:/etc/rhsm/rhsm.confSet timezonevirt-customize -a Rocky-8-GenericCloud-Base.latest.x86_64.qcow2 --timezone \"America/New_York\"Add SSH keyvirt-customize -a Rocky-8-GenericCloud-Base.latest.x86_64.qcow2  --ssh-inject jmutai:file:./id_rsa.pubRunning on first boot onlyI found that the first time my cloud-init configured instance booted I wanted it to finish configuring and then reboot itself one time to register the IP address properly in DNS. Use virt-edit and then add something at the bottom of the file like this to reboot the machine 60 seconds after cloud-init finishes.power_state:    delay: 1    mode: reboot    message: Rebooting machine    condition: trueCreating a VM template in ProxmoxI use Proxmox as my open-source hypervisor so this guide will also cover creating a VM template from this image.We do this by SSHing into Proxmox and executing the following commands.Create a VM with ID 8000, 8GB memory, 4 cores, name it rocky-cloud and give it basic networking.qm create 8000 --memory 8192 --core 4 --name rocky-cloud --net0 virtio,bridge=vmbr0Import the image. vm-storage would be the name of the local storage you want to host this VM. Proxmox default storage location is local-lvm.qm importdisk 8000 Rocky-8-GenericCloud-Base.latest.x86_64.qcow2 vm-storageCreate a scsi controller to handle the disks and attach the image disk.qm set 8000 --scsihw virtio-scsi-pci --scsi0 vm-storage:vm-8000-disk-0attach the CD drive as a cloud-init drive.qm set 8000 --ide2 vm-storage:cloudinitSet the image disk to boot first.qm set 8000 --boot c --bootdisk scsi0Increase storage by desired size.qm resize 8000 scsi0 +15GConvert to templateqm template 8000Last stepsNow that we have a working template go into proxmox and find the template we created. Now you can click \"cloud-init\" and edit the final settings. Here we edit the default username and password, SSH keys, and it is extremely important to set the DHCP to dynamic otherwise the host will not grab networking. I add my domain in the DNS domain field. I also go under options and check the box for QEMU guest agent = enabled.  For Rocky Linux the VM will not work on proxmox unless you also go under hardware and select processors and under type make your type at the very bottom “host”.Once all this is done you can now use this template by right clicking it and selecting clone. Full clone option will make the clone's hard drive entirely independent and is preferred. Once the machine is ran cloud-init will configure it and after 5 minutes or so you should be able to connect to it via SSH/login."
  },
  
  {
    "title": "Apache Guacamole for a remote lab",
    "url": "/sysblob.github.io/posts/guacamole/",
    "categories": "homelabbing",
    "tags": "guacamole, remote, ssh, rdp",
    "date": "2023-09-04 00:00:00 -0400",
    





    
    "snippet": "Apache Guacamole is a browser based experience for remote SSH and RDP access. In a nut shell you run a self-hosted server which you connect to via a web GUI. From within this site you can add conne...",
    "content": "Apache Guacamole is a browser based experience for remote SSH and RDP access. In a nut shell you run a self-hosted server which you connect to via a web GUI. From within this site you can add connections to your various networked devices - and they work right there in your browser. Guacamole supports many connection types and encryption protocols so you’re sure to find what you need.An IntroductionLet’s jump right in and take a look at some of the interface of Guacamole.Guacamole has a clean interface for quickly getting at your saved connections. It even features a preview mode in each box so you can have an idea of what you’re connecting to. Guacamole supports the following protocols:  Kubernetes  RDP  SSH  Telnet  VNCFor SSH Guacamole supports username and password based authentication or SSH keys. If you plan on using SSH though, see the note below.  For SSH key algorithms Guacamole is very picky. You’re required to use PEM format. To generate a key compatible with Guacamole try “ssh-keygen -t rsa -b 4096 -m PEM”Guacamole allows for User management and has some minimal settings. No distractions here from adding connections and getting going.When adding an SSH connection Guacamole wants you to specify your key in the OpenSSH format as shown. Guac allows for some terminal customization if you prefer a certain color when you hack away. I think the green on black looks the smoothest as seen below.Now that we’ve taken a look at the straight forward settings of Guacamole. Let’s go through setting up a Guacamole server.SetupThis installation is based off a fresh Ubuntu 22.04 server.Installing GuacdGuacamole has a lot of dependencies based on what connections you intend to run. Let’s install the usual suspects.sudo apt install build-essential libcairo2-dev libjpeg-turbo8-dev libpng-dev libtool-bin uuid-dev libavcodec-dev \\libavformat-dev libavutil-dev libswscale-dev freerdp2-dev libpango1.0-dev \\libssh2-1-dev libtelnet-dev libvncserver-dev libwebsockets-dev \\libpulse-dev libssl-dev libvorbis-dev libwebp-devNext let’s download Guacamole to our server.wget https://downloads.apache.org/guacamole/1.5.2/source/guacamole-server-1.5.2.tar.gzExtract the file and navigate to its directory.tar -xvf guacamole-server-1.5.2.tar.gzcd guacamole-server-1.5.2Build the installation based off the source files.sudo ./configure --with-init-dir=/etc/init.d --enable-allow-freerdp-snapshotssudo makesudo make installUpdate installed library cache and reload systemd.sudo ldconfigsudo systemctl daemon-reloadStart Guacd and enable it to start at boot.sudo systemctl start guacdsudo systemctl enable guacdCreate a directory to store Guacamole configuration files and extensions. These directories are used in later steps.sudo mkdir -p /etc/guacamole/{extensions,lib}Installing TomcatInstall Apache Tomcat and modules.sudo apt install tomcat9 tomcat9-admin tomcat9-common tomcat9-userDownload the Guacamole client.wget https://downloads.apache.org/guacamole/1.5.2/binary/guacamole-1.5.2.warMove the client to the Tomcat web directory.sudo mv guacamole-1.5.2.war /var/lib/tomcat9/webapps/guacamole.warRestart both Apache Tomcat and Guacd.sudo systemctl restart tomcat9 guacdSetting up a DatabaseWhile Apache Guacamole does support basic user authentication via a user-mapping.xml file, it should only be used for testing. For this guide, we will use production-ready database authentication through MySQL/MariaDB.Install either MySQL or MariaDB on your system. (This guide follows MySQL)sudo apt install mysql-serverRun the following commands to perform the initial security configuration:sudo mysqlALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY 'SetRootPasswordHere';exitsudo mysql_secure_installationBefore populating the database, we need to install a few things. Mainly we need to install the MySQL Connector/J library and Guacamole JDBC authenticator plugin.Download the MySQL Connector/J (Java Connector). For this guide, download the platform independent archived file.wget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-8.0.26.tar.gzExtract the tar file and copy it to /etc/guacamole/lib/.tar -xf mysql-connector-java-8.0.26.tar.gzsudo cp mysql-connector-java-8.0.26/mysql-connector-java-8.0.26.jar /etc/guacamole/lib/Download the JDBC auth plugin for Apache Guacamole. This file can be found on http://guacamole.apache.org/releases/ by selecting the release version and then locate the “jdbc” file.wget https://downloads.apache.org/guacamole/1.5.2/binary/guacamole-auth-jdbc-1.5.2.tar.gzExtract the tar file and copy it to /etc/guacamole/extensions/.tar -xf guacamole-auth-jdbc-1.5.2.tar.gzsudo mv guacamole-auth-jdbc-1.5.2/mysql/guacamole-auth-jdbc-mysql-1.5.2.jar /etc/guacamole/extensions/Log in to mysql as the root user.mysql -u root -pThe prompt should change again to mysql&gt;.While in the mysql prompt we run the commands below. The goal is to change the root password, create a database, and create a new user for that database. When running the commands, replace any instance of password with a secure password string for the mysql root user and the new user for your database, respectively.ALTER USER 'root'@'localhost' IDENTIFIED BY 'password';CREATE DATABASE guacamole_db;CREATE USER 'guacamole_user'@'localhost' IDENTIFIED BY 'password';GRANT SELECT,INSERT,UPDATE,DELETE ON guacamole_db.* TO 'guacamole_user'@'localhost';FLUSH PRIVILEGES;Exit the MySQL prompt by typing quit.Locate the scheme files in the extracted directory for the JDBC plugin.cd guacamole-auth-jdbc-1.5.2/mysql/schemaImport those sql schema files into the MySQL database.cat *.sql | mysql -u root -p guacamole_dbCreate the properties file for Guacamole.sudo nano /etc/guacamole/guacamole.propertiesPaste in the following configuration settings, replacing [password] with the password of the new guacamole_user that you created for the database.# MySQL propertiesmysql-hostname: 127.0.0.1mysql-port: 3306mysql-database: guacamole_dbmysql-username: guacamole_usermysql-password: [password]Restart all related services.sudo systemctl restart tomcat9 guacd mysqlAll doneGuacamole should now be accessible at:[ip]:8080/guacamoleConnection tipsI’ve discovered a couple quirks when it comes to setting up Guacamole connections. Here are some tips.  For Windows RDP connections set the security mode to NLA Authentication  For both linux and windows connections make sure to check the box to ignore certificate warnings  For SSH the entry only requires hostname, port 22, your username, and the SSH key in the format seen below.  I’ve found Guacamole doesn’t seem to do well with DNS so I use IP addresses. This could be my own issues."
  },
  
  {
    "title": "A deep dive into Docker",
    "url": "/sysblob.github.io/posts/docker/",
    "categories": "development",
    "tags": "docker, containers, devops",
    "date": "2023-09-02 00:00:00 -0400",
    





    
    "snippet": "Docker is a great straight forward way of using containerization. I’m a big fan, and use Docker to pretty much use Docker to run most of my lab.Refer to the Table of Contents if you are looking for...",
    "content": "Docker is a great straight forward way of using containerization. I’m a big fan, and use Docker to pretty much use Docker to run most of my lab.Refer to the Table of Contents if you are looking for something specific about Docker.What is Docker?The brand Docker has become synonymous with containers like Kleenex is to tissues. The reason for Docker’s popularity is it makes running containers a whole streamlined ecosystem. While lots of really cool virtualization technology is taking place behind the scenes, the user’s experience is straight forward. Write a docker compose file which has all your components, point to an image on dockerhub to use for a baseline, tell it what ports it can use, and fire it up. Docker is great, but why do we even use containers? While this Docker guide isn’t meant to be an explanation of containers in general, let’s make sure we’re clear on the basics.Most people know how a physical computer works. It has CPU, RAM, Storage, other hardware, and an operating system. Virtual machines take this concept further by installing something called a Hypervisor. Hypervisors can be classified in two forms. They can either be installed directly onto the hardware as you would an operating system, we call these baremetal hypervisors or type 1. VMWare’s ESXi or Proxmox would be example’s of a type 1 hypervisor. Or they can be installed on top of the operating system which we call type 2. VMWare’s VirtualBox is an example of a type 2 hypervisor. From these hypervisors you are able to allocate a fraction of your real physical resources to create a virtual machine.In this sense a Virtual Machine is a complete emulation of a physical machine. It virtualizes everything right down to the hardware creating virtual cpus, virtual storage devices, a virtual network card, and everything else a computer needs to exist. Obviously this can be very taxing on resources since you have to virtualize so much. This is what brought about the necessity for containers.If virtual machines virtualize the hardware of physical machines then containers could be thought of as virtualizing an operating system. Containers are extremely lightweight, can start instantly, and have a footprint of mere Megabytes compared to Virtual Machines Gigabytes. Containers are able to perform this by sharing the operating system kernel. In other words, linux containers can only exist on a linux host, and windows containers can only exist on a windows host. The advantage here is you can then package a linux application with the bare minimum it takes to run, and it will run on any linux machine. Docker is a great and easy way of managing this container ecosystem.If further understanding of virtualization is necessary this video goes into good concepts of not only what VMs and containers are but how the need for them developed. This need is first and foremost resource utilization. It’s easier to see once you jump into it. The next section is installing Docker.One last note on virtualization. We don’t live in a world where we have to choose between virtual machines and containers. Use both! The great thing about these two technologies is they work well together. If you create a virtual machine and then install docker onto it and run containers, you can then use the hypervisor to take snapshots of the VM and have full control over the environment docker is installed on.Setup  This is a How-to guide on setting up Docker on Ubuntu Linux.These commands are taken directly from Docker documentation and explained for your convenience.      Uninstall old docker versions to start fresh     sudo apt-get remove docker docker-engine docker.io containerd runc            Add Docker’s GPG key to your machine. This acts as a signature for the docker repo and lets your machine know to trust it. It also provides encryption during the file transfer process.     curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg            Next, use the following series of commands to send a script block to the terminal which points to the public GPG key that is now on the machine and tells the apt package manager to include the docker repository as a 3rd party trusted repo.     echo \\   \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\   $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null            Update the apt package index to include the new docker repo and install docker along with its engine and docker compose.     sudo apt update         sudo apt install docker-ce docker-ce-cli containerd.io docker-compose-plugin            Verify docker is working as intended by running the test docker image.     sudo docker run hello-world            Add your current logged in user to the Docker linux group which has permissions equivalent to root for editing file systems for your containers. (If this group is missing you can use sudo groupadd docker)     sudo usermod -aG docker $USER            You can verify it worked by logging back in using su -- username then trying to run docker without sudo now.     docker run hello-world            On ubuntu Docker is added to start automatically but on other distributions you may need to use systemd the default system manager of most linux machines.     sudo systemctl enable docker.service         sudo systemctl enable containerd.service        By default docker creates log files which can grow in size infinitely slowly taking away system resources. We correct this by adding log rotation. Either create or edit the file /etc/docker/daemon.json which is what controls daemon settings for docker. Add the following lines to setup log rotation with a max amount of 3 files at 50MB a piece before overwriting. We are also specifying that the log output should be in json. The JSON format annotates each line with its origin (stdout or stderr) and its timestamp. Each log file contains information about only one container.    {      \"log-driver\": \"json-file\",      \"log-opts\": {        \"max-size\": \"50m\",        \"max-file\": \"3\"       }    }UsageDocker has a couple terms you should be familiar with:  DockerHub: The official Docker repository of premade images.  Image: The lightweight OS and dependencies downloaded from DockerHub which are needed for an application.  Bind: Docker images will often reference a port or a file directory inside and outside the container. For example “80:8080” in docker speak would be a bind for port 80 on the host to port 8080 inside the container.  Docker Compose: While you can run a single container by executing commands and referencing an image, you can also create a series of instructions for running one or many containers. This is called a compose file. Compose files are really the best way to re-use your images and containers.Let’s take a look at a docker-compose.yml file which can be used to create a self-hosted dashboard website called Homerr.version: \"2\"services:  homer:      image: b4bz/homer      container_name: homer      volumes:        - /home/lucyadmin/docker/config/homer:/www/assets      ports:        - 8092:8080      #environment:      #  - UID=1000      #  - GID=1000      restart: unless-stoppedLet’s break down this compose file:  Version: \"2\" - You will see versioning in a lot of Docker compose files but this is a deprecated practice. I used this as an example.  Services: - specify what services are running in your compose stack.  homer: - the container we are going to be using.  image: b4bz/homer - this tells the compose file where to download the image file from DockerHub  container_name: - specify name the container will be referenced by from now on  volumes: - Docker uses volumes to store its data. For simplicity we use binds here.  /home/lucyadmin/homer:www/assets - This bind is saying the files that are located in /home/lucyadmin/homer are mounted inside the container at the location www/assets.  ports: - We can specify what ports are outside the container  8092:8080 - Here we are saying port 8092 on the host machine is bound to 8080 inside the container  UID/GUID: - You can typically specify in a compose file which user or group will have permissions over the files written into the volumes.  restart: unless-stopped - Ensures docker container is started upon reboot of the host machine.We launch a docker compose stack by creating the yml file and launching it with the compose up command. The -d portion stands for detached and it makes it so the compose stack runs in the background in its own terminal.docker compose up -dYou now should be able to access your server by going to the hostname:portnumber in your web browser.This is using compose, however you can use Docker directly to spin up containers too. Here are some general Docker use commands.Running containersUse the run command to start a new container from an image:docker run nginxassign it a name:docker run --name web nginx # web is the name and nginx the containermap a port:docker run -p HOSTPORT:CONTAINERPORT IMAGEdocker run -p 8080:80 nginxrun the container as a background process:docker run -d nginxassign it a hostname:docker run --hostname svr01 nginxadd a dns entry:docker run --add-host HOSTNAME:IP IMAGEmap a local directory into the container:docker run -v HOSTDIR:TARGETDIR IMAGEdocker -v ~/:/usr/share/nginx/html nginxManaging containerslist containers:docker ps # show running containersdocker ps -a # show all containersdelete a container:docker rm containername # you can add -f if container is running to forcestart/stop a running container:docker stop containernamedocker start containernamecopy file between docker:docker cp containername:filename filename # docker to hostdocker cp filename containername:filename # host to dockerrename a container:docker rename oldname newnamestart shell inside container:docker exec -it containername bashManaging imagesdownload an image:docker pull imagenamedelete an image:docker rmi imagenamelist all images:docker imagesInformational and Utilitycheck version of docker:docker versionshow stats of running containers:docker statsshow processes of a container:docker top containernameshow mapped ports of a container:docker port containernameDocker loggingConfiguring docker logsBy default docker creates log files which can grow in size infinitely slowly taking away system resources. We correct this by adding log rotation. Either create or edit the file /etc/docker/daemon.json which is what controls daemon settings for docker. Add the following lines to setup log rotation with a max amount of 3 files at 50MB a piece before overwriting. We are also specifying that the log output should be in json. The JSON format annotates each line with its origin (stdout or stderr) and its timestamp. Each log file contains information about only one container.{  \"log-driver\": \"json-file\",  \"log-opts\": {    \"max-size\": \"50m\",    \"max-file\": \"3\"   }}Docker log locationIf you come across an issue where you need to delve into Docker logs they can be located on the host where the container is running. First obtain your docker container ID, then look for the log found here:/var/lib/docker/containers/&lt;containerid&gt;Using log commandslist all running containers, use the docker ps command.docker psThen, with the docker logs command you can list the logs for a particular container.docker logs &lt;container_id&gt;Most of the time you’ll end up tailing these logs in real time, or checking the last few logs lines.Using the –follow or -f flag will tail -f (follow) the Docker container logs:docker logs &lt;container_id&gt; -fThe –tail flag will show the last number of log lines you specify:docker logs &lt;container_id&gt; --tail NThe -t or –timestamp flag will show the timestamps of the log lines:docker logs &lt;container_id&gt; -tThe –details flag will show extra details about the log lines:docker logs &lt;container_id&gt; --detailsBut what if you only want to see specific logs? Luckily, grep works with Docker logs as well.docker logs &lt;container_id&gt; | grep patternThis command will only show errors:docker logs &lt;container_id&gt; | grep -i errorOnce an application starts growing, you tend to start using Docker Compose. Don’t worry, it has a logs command as well.docker compose logsThis will display the logs from all services in the application defined in the Docker Compose configuration file.Cheat Sheet"
  },
  
  {
    "title": "This website is made with Jekyll",
    "url": "/sysblob.github.io/posts/jekyll/",
    "categories": "development",
    "tags": "homelabbing, website, jekyll, github",
    "date": "2023-08-23 00:00:00 -0400",
    





    
    "snippet": "IntroductionI’ve spent a lot of time building websites. When I was wee youngster I remember the thrill of logging into free template providers like Geocities or Angelfire that made it so easy. I wa...",
    "content": "IntroductionI’ve spent a lot of time building websites. When I was wee youngster I remember the thrill of logging into free template providers like Geocities or Angelfire that made it so easy. I was 12 or 13 at the time and mostly just wanted a website with a cool flame.gif border. As I became older I got interested in learning the code behind websites and studied HTML, CSS, and Javascript. As my awareness of developing grew I got into web frameworks like Django, Flask, and React. Building websites is something I really do enjoy. I think that’s why when I was introduced to the Github world of website building, I was excited to try another method of managing the whole process. Github brings a lot of organization to coding and websites are after all just more code – why not manage them directly from Github?What is Jekyll?Jekyll https://jekyllrb.com/ is a static website generator made by Github. Jekyll takes plain text and markdown and converts it into beautiful websites. It does this by rendering html from your markdown and combining that with what we call a theme.Themes give your pages a layout such as a blog or a resume site. Themes typically come with config files that are easily edited to customize your website. Beyond that there are certain directories you can ‘override’ files such as css to suit your needs. Themes are based on Ruby gems and can be easily swapped out once you set Jekyll up.This website, for example, uses Jekyll to generate static content using the theme called Chirpy found here https://github.com/cotes2020/jekyll-theme-chirpy.What is Github Pages?Github Pages https://pages.github.com/ was created with the idea of hosting a static website directly from your Github repository. Essentially as long as you have an index.html file at the base of your website directory you can push to github and host a website. Jekyll works great for this since you can write something like a blog post in plain text, mark it up with how you’d like it to look, generate the real website from that markdown, and upload it to Github.Does this interest you? Let’s build this really cool pipeline.Setting up Jekyll with Chirpy themeI’m still working on this portion of the guide. Check back soon sorry for the delay.Jekyll usage and tipsHere you will find some general guidelines on using Jekyll. For the most part Jekyll and Chirpy have great documentation so I will just be linking to them below. However, for some specific things I had to search a little harder for you can find those below as well.Jekyll docs: Jekyll page buildingChirpy docs: Chirpy formattingHow do I link to another post?You can link another post internally by referencing it like below:[link name here]({% post_url 2023-09-02-postname %})Using codeblock around template languageSometimes you will encounter wanting to codeblock something that liquid (the template renderer of Jekyll) wants to render. You can avoid this and still make a code block by using raw and endraw around the codeblock."
  },
  
  {
    "title": "Homelabbing (Sysblob.com)",
    "url": "/sysblob.github.io/posts/myhomelab/",
    "categories": "homelabbing",
    "tags": "showcase, rack, servers",
    "date": "2023-08-23 00:00:00 -0400",
    





    
    "snippet": "Hello World – and welcome to Sysblob.com, a website dedicated to homelabbing. I’ve been a big fan of tinkering and computers for a long time, and I’ve made a career of it as a Linux Administrator. ...",
    "content": "Hello World – and welcome to Sysblob.com, a website dedicated to homelabbing. I’ve been a big fan of tinkering and computers for a long time, and I’ve made a career of it as a Linux Administrator. However, around 4 years ago I decided to take my homelabbing game to another level. Fed up with never getting to physically touch an enterprise server, I purchased a Dell PowerEdge R730, a rack, and began an adventure of learning. Since then I’ve expanded to a 2nd PowerEdge server, 24 port switch and router, cloud servers, and countless other devices. My labbing interests span from multimedia to enterprise infrastructure, automation, networking, coding and development, security, AI, and so much more. I like to keep a healthy balance of real world enterprise experience with stuff that’s just plain fun – and that’s what you’ll find here!The goal of Sysblob.com is three fold:  To teach concepts that can be difficult to grasp in a more direct and human consumable way  To provide direct copy and paste how-to guides on performing setup and configuration items  To showcase my more fun homelab projectsMy labNormally this is the part where I would give a glimpse into some of the current infrastructure and services of my lab. However things are currently in disarray as I’m in the process of moving. Once everything is back to order I’ll update.My hardware  x2 Dell PowerEdge r730 8 LFF (2)E5-2660v3 2.60Ghz 64GB H730 Perc (4x)1G (2)750W (One of these runs an nvidia geforce 1070)  Beelink SEI8 Mini Intel 8th Gen 8109U(up to 3.6Ghz), 16GB DDR4 RAM 500GB NVMe SSD, Gigabit Ethernet  24 port Mikrotik switch CRS326  Protectli Vault 4 Port, Firewall Micro Appliance/Mini PC - Intel Quad Core, AES-NI, 8GB RAM, 120GB mSATA SSD  x2 HP ProDesk 600 G3 SFF 3.6 GHz Intel Core i7-7700 Quad-Core, 64 GB DDR4 with vPro with 1 TB SSDs  Waveshare 11.9inch Capacitive Touch Screen LCD (This is the screen in my rack if you’re curious)  12U adjustable StarTech Rack  Synology NAS 920+ with x5 8TB drives and +8GB additional ram (16)  Raspberry Pi 4B  TP-Link EAP670 Omada WiFi 6 AX5400 Wireless 2.5G Ceiling Mount  2022 Mac M1 Pro (Daily driver)Infrastrcuture  Proxmox  VMWare ESXi  Opnsense  Semaphore (Ansible)  Foreman/Katello/Puppet  FreeIPA  2022 Active Directory  Cloudflare DNS and tunneling  Internal DNS via Unbound  Docker/Portainer  Apache Guacamole  Wireguard VPN  Tailscale Overlay networkLogging/Monitoring  Greylogs  Grafana  Uptime KumaHosted applications  Bookstack  Portainer  Plex  Radarr, Sonarr, Lidarr, Prowlarr, Overseerr  Tautulli  Transmission  Sabnzbd  Snippetbox  Metube  Organizr  Nextcloud  HomerrDiagramsComing soon."
  },
  
  {
    "title": "Everything Bookstack",
    "url": "/sysblob.github.io/posts/bookstack/",
    "categories": "homelabbing",
    "tags": "bookstack, documentation, self-hosted",
    "date": "2023-08-21 00:00:00 -0400",
    





    
    "snippet": "Bookstack is a self-hosted wiki that makes editing and storing your documentation in an organized and secure fashion fast, efficient, and easy.Link: https://www.bookstackapp.com/Refer to the Table ...",
    "content": "Bookstack is a self-hosted wiki that makes editing and storing your documentation in an organized and secure fashion fast, efficient, and easy.Link: https://www.bookstackapp.com/Refer to the Table of Contents if you are looking for something specific about Bookstack.What is Bookstack?Bookstack is first and foremost a self-hosted wiki. A place to store all your knowledge and documentation so you don’t lose it or forget it. Unlike static website generators, however, Bookstack is a fully fledged app. You create, edit, publish, and admin over, all your content directly from Bookstack’s Web GUI. This makes it more like a Word Press designed for documentation.Bookstack can be setup as a stand alone apache web server, or a docker container. Luckily, Bookstack is an easy installation – as the creator has made many scripts available to the community. I personally run both of my Bookstack instances (one local, one remote) as stand alones as I gave them their own virtual machines.Let’s take a look at the interface of Bookstack and see what makes it unique.You’ll notice right away the unique way Bookstack displays information. Your documentation is laid out in a hierarchy of Shelfs &gt; Books &gt; Chapters &gt; Pages. You can use as much or as little of this hierarchy as you want to. To give you an idea of my layout I have two shelves, “Applications”, and “Commands and Processes”. Under My applications shelf for example I have a Book Docker. I do not use Chapters, but I then have several pages on Docker, i.e. “Docker setup”, and “Docker logging”.You’ll also notice the customization. Bookstack has a settings area which allows for colors and themes. You can upload logos, and if you’re really ambitious, you can add CSS overrides or edit the code as you wish.Bookstack also has built in user management which is nice. You can add users, authenticate them via email, delegate what rights they have and over what pages, and much more. Other features Bookstack supports are Webhooks to send out data on post actions, and a built in audit log if you want to have a bit of user accountability.Where Bookstack really shines though, is the ease at which you can create content. Just go where you want, click new page, and everything you need to create and publish a new post is right there. From font customization and coloring, code blocks with syntax highlighting, tables, graphs, images, videos, the whole tool kit is there.The best part is Bookstack uses a database and a few files which can easily be backed up so you can take your content on the go pretty easily. The developer even included a script to backup and restore which I’ll cover.I highly recommend Bookstack as an open source free project to store your documents or blog.Setting up BookstackYou can install Bookstack via an automated script which makes it easy, or you can use a docker stack. Let’s see the script method first.Automatic install (Requires fresh Ubuntu 22.04)To utilize the script we download it, make sure it has the right permissions, edit any lines we need, and run it. Series of commands:wget https://raw.githubusercontent.com/BookStackApp/devops/main/scripts/installation-ubuntu-22.04.shchmod a+x installation-ubuntu-22.04.shnano installation-ubuntu-22.04.sh # edit the script if you're importing an old bookstacksudo ./bookstack-install.sh  If your plan is to import your old bookstack look to comment out the line which says “php artisan migrate –no-interaction –force”Docker InstallationIf you need help setting up Docker see my guide here: A deep dive on DockerMake a docker-compose.yml file and paste in this content:---version: \"2\"services:  bookstack:    image: lscr.io/linuxserver/bookstack    container_name: bookstack    environment:      - PUID=1000      - PGID=1000      - APP_URL=https://bookstack.example.com      - DB_HOST=bookstack_db      - DB_PORT=3306      - DB_USER=bookstack      - DB_PASS=&lt;yourdbpass&gt;      - DB_DATABASE=bookstackapp    volumes:      - ./bookstack_app_data:/config    ports:      - 6875:80    restart: unless-stopped    depends_on:      - bookstack_db  bookstack_db:    image: lscr.io/linuxserver/mariadb    container_name: bookstack_db    environment:      - PUID=1000      - PGID=1000      - MYSQL_ROOT_PASSWORD=&lt;yourdbpass&gt;      - TZ=Europe/London      - MYSQL_DATABASE=bookstackapp      - MYSQL_USER=bookstack      - MYSQL_PASSWORD=&lt;yourdbpass&gt;    volumes:      - ./bookstack_db_data:/config    restart: unless-stoppedContainer images are configured using parameters passed at runtime (such as those above). Binding is when we tell docker we want ports or directories to be represented inside the container directly. For example, -p 8080:80 would expose port 80 from inside the container to be accessible from the host’s IP on port 8080 outside the container. This container uses 6875 externally and can be reached at – http://hostname:6875PUID and GUID should be set to your user that will be owning the volume directories for Docker. You can check your ids in linux by running command “id username”.If you need help specifying timezone see this link: Timezone ListYou can then run the command:docker compose up -dUsage and TipsBacking up BookstackIn simplified terms you really only need to backup a couple files as well as export the MySQL database.cd /var/www/bookstacksudo mysqldump -u bookstack bookstack &gt; bookstack.backup.sql # -u bookstack is the name of your database owner found in /var/www/bookstack/.envtar -czvf bookstack-files-backup.tar.gz .env public/uploads storage/uploads bookstack.backup.sqlNow move this file where ever you need.Bookstack also comes with a great script for backing up your stack.Usage  Copy the script down to a file (bookstack-backup.sh).  Tweak the configuration variables at the top of the script.  Make the script executable (chmod +x bookstack-backup.sh).  Run the script (./bookstack-backup.sh).Here is the script:#!/bin/bash# Directory to store backups within# Should not end with a slash and not be stored within # the BookStack directoryBACKUP_ROOT_DIR=\"$HOME\"# Directory of the BookStack install# Should not end with a slash.BOOKSTACK_DIR=\"/var/www/bookstack\"# Get database options from BookStack .env fileexport $(cat \"$BOOKSTACK_DIR/.env\" | grep ^DB_ | xargs)# Create an export name and locationDATE=$(date \"+%Y-%m-%d_%H-%M-%S\")BACKUP_NAME=\"bookstack_backup_$DATE\"BACKUP_DIR=\"$BACKUP_ROOT_DIR/$BACKUP_NAME\"mkdir -p \"$BACKUP_DIR\"# Dump database to backup dir using the values# we got from the BookStack .env file.mysqldump --single-transaction \\ --no-tablespaces \\ -u \"$DB_USERNAME\" \\ -p\"$DB_PASSWORD\" \\ \"$DB_DATABASE\" &gt; \"$BACKUP_DIR/database.sql\"# Copy BookStack files into backup dircp \"$BOOKSTACK_DIR/.env\" \"$BACKUP_DIR/.env\"cp -a \"$BOOKSTACK_DIR/storage/uploads\" \"$BACKUP_DIR/storage-uploads\"cp -a \"$BOOKSTACK_DIR/public/uploads\" \"$BACKUP_DIR/public-uploads\"# Create backup archivetar -zcf \"$BACKUP_DIR.tar.gz\" \\ -C \"$BACKUP_ROOT_DIR\" \\ \"$BACKUP_NAME\"# Cleanup non-archive directoryrm -rf \"$BACKUP_DIR\"echo \"Backup complete, archive stored at:\"echo \"$BACKUP_DIR.tar.gz\"Restoring BookstackThis restore process works best on a fresh Bookstack installation and refers to the directories of the standalone setup.After moving your tar file over from the backup process unzip it.tar -xvzf bookstack-files-backup.tar.gzMove the files into appropriate directories.cp -r public/uploads/* /var/www/bookstack/public/uploadscp -r storage/uploads/* /var/www/bookstack/storage/uploads  It’s important the APP_Key value remains the same between old and new bookstack or it can break things.Open the old .env file and copy the APP_KEY value and paste it into new Bookstacks .envnano .env # and copy keynano /var/www/bookstack/.env # paste keyFinally while you’re in the new Bookstack’s .env file make sure to specify the new app URL in APP_URL.APP_URL=https://sysblob.comImport the old database into your new running Bookstack.mysql -u bookstack -p bookstack &lt; bookstack.backup.sqlRun the first time database setupcd /var/www/bookstack; php artisan migrate --no-interaction --forceFinally it’s important if you changed the APP_URL field to update the database so it knows.php artisan bookstack:update-url http://old.address.com https://new.address.comLocating your .env fileYour .env file handles a lot of configuration for Bookstack. This is located at /var/www/bookstack/.envBookstack inside an iFrameBy default BookStack will only allow itself to be embedded within iframes on the same domain as you’re hosting on. This is done through a CSP: frame-ancestors header. You can add additional trusted hosts by setting an ALLOWED_IFRAME_HOSTS option in your .env file like the example below:# Adding a single hostALLOWED_IFRAME_HOSTS=\"https://example.com\"# Multiple hosts can be separated with a spaceALLOWED_IFRAME_HOSTS=\"https://a.example.com https://b.example.com\"  When this option is used, all cookies will be served with SameSite=None (info) set so that a user session can persist within the iframe.Setting default Dark modeOpen the file at /var/www/bookstack/.env and add in this line:APP_DEFAULT_DARK_MODE=trueDefault file permissionsIf you’re having any issues with file permissions here is the exact quote taken from Bookstack on a typical file setup using the user Barry.  Set the bookstack folders and files to be owned by the user barry and have the group www-data    sudo chown -R barry:www-data /var/www/bookstack        Set all bookstack files and folders to be readable, writeable &amp; executable by the user (barry) and readable &amp; executable by the group and everyone else    sudo chmod -R 755 /var/www/bookstack        For the listed directories, grant the group (www-data) write-access    sudo chmod -R 775 /var/www/bookstack/storage /var/www/bookstack/bootstrap/cache /var/www/bookstack/public/uploads        Limit the .env file to only be readable by the user and group, and only writable by the user.    sudo chmod 640 /var/www/bookstack/.env      "
  },
  
  {
    "title": "Plex and the *ARR stack",
    "url": "/sysblob.github.io/posts/plex/",
    "categories": "homelabbing",
    "tags": "plex, sonarr, radarr, lidarr, prowlarr, sabnzbd, transmission",
    "date": "2023-08-18 00:00:00 -0400",
    





    
    "snippet": "Plex is a media management and streaming service akin to a self-hosted netflix. While Plex by itself is great, it’s common to design something around plex for the downloading and obtaining of media...",
    "content": "Plex is a media management and streaming service akin to a self-hosted netflix. While Plex by itself is great, it’s common to design something around plex for the downloading and obtaining of media called an *ARR stack. ARR referring to the suite of products – Radarr (Movies), Sonarr (TV), Prowlarr (Indexer), Lidarr (Music), and more. Let’s take a look at a basic plex stack so we can learn some concepts.PlexPlex is the bread and butter of the media ecosystem. Plex’s primary role is streaming content to users, but it also performs other functions. Plex will reach out to IMDB and Rotten Tomatoes and download meta data for your media, scan TV shows you add for intros and setup skips for them, download cover art, and even setup movie trailers. Plex is served as an app and can be found on any major app store on your computer, phone, or smart TV.The Plex server itself can be setup in dozens of ways – but my preferred way is on a Synology NAS, specifically a 4 bay Synology NAS 920+ model. The advantages of this device are plentiful:  Storage: This model comes with 4 bays which means Terabytes worth of storage for your movies and tv shows. I run mine currently with x4 8TB drives.  Transportable: Light weight and easy to transport if you need to move your media with you somewhere new.  Small footprint: This unit is small, relatively quiet, and can just sit on a shelf somewhere.  Built in conveniences: Built in apps, data management, file sharing services, and of course, a built in plex media app that works right out of the box.  Transcoding: This is the reason for this particular model. See below.Transcoding is what takes place when Plex tries to stream to a user, but realizes the device they are on may be incompatible with the media you’re streaming. It then converts or ‘transcodes’ the media into something compatible. This can take lots of processing power or typically a GPU – but this NAS has it built in which is what makes it so great.This is just one of many options though. Plex can be run off of nearly anything, particularly if you don’t plan on transcoding. If you aim to make your media collection compatible and especially if you’re streaming locally from home, you can avoid transcoding entirely. This is the preferred method. But hey, if you need it you always want to have it available – which is why the 920+ is a great choice.Let’s look at some of the *ARR next.Radarr, Sonarr, and LidarrRadarr is the application in your stack that will deal in movies. Radarr, Sonarr (TV), and Lidarr (Music) are built to look and feel pretty much identical. They all are installed and accessed via a local web server which serves out on a particular port. The defaults are:  Radarr: 7878  Sonarr: 8989  Lidarr: 8686Here is a peek at Radarr’s interface which again is identical to the other ARR:Radarr’s purpose (and the other ARR for their particular media type) is to manage a couple primary things:  It uses your index manager (Prowlarr) to take a list of websites and search through them for media that matches your criteria. More on indexers later.  It sends the list that it finds to your downloading apps, which is typically Sabnzbd for Usenet and Transmission for Torrents. More on downloaders later as well.  It also renames your files into a naming scheme you specify once downloaded – such as naming movie files with a date or creating folders for TV seasons.  Finally it moves the files from your download folder into your appropriate plex folder to be viewedLet’s look at Prowlarr another important piece.ProwlarrMany people refer to Prowlarr as an indexer, but technically it manages your indexers. An indexer can be thought of as a website that houses a large amount of information on where files are located and their meta data. It stores this information in a format that programs like Radarr or Sonarr can then search through. Let’s take a look at the Prowlarr interface.As you can see from the screenshot it’s important to have multiple indexers loaded into Prowlarr. The more indexers you have the more locations you can search for the movie or show you’re looking for. In Prowlarr, indexer information is entered such as login information and urls, and prowlarr distributes this information to your *ARR stack.At this point we know the general flow. Prowlarr feeds indexer urls and login information to radarr and sonarr, radarr and sonarr use these sites to search for what you want, but what after that? Well of course we need downloading apps. Before we get into that though, let’s cover Usenet vs Torrenting.Usenet vs TorrentingWhen it comes to downloadable content it pretty much exists in 2 places – Usenet and Torrenting. These two methods of downloading are very different.TorrentingTorrenting is downloading a file from an indexer which then directs you to multiple hosts known as seeders which are hosting that file. You then download from those hosts, often people’s own machines who downloaded just like you, pieces of the overall file simultaneously.Advantages of Torrenting:  Often files that are difficult to find on Usenet can be found torrenting  Older content can often be found on torrentingDisadvantages of Torrenting:  Since you’re downloading from people instead of a dedicated server torrents will always be slower  Torrents require you to pay for VPN service in order to secure your connection to peers  Due to the requirement of using a VPN the setup can be a bit more difficultUsenetUsenet downloads bits of a file from many dedicated servers then puts it all together. These download servers operate on a newsgroup model which is the way the internet operated before there was an internet. These servers are interconnected in a way such that if you upload a file to one, it uploads to all of them. Much like how you subscribe to a modern internet provider, you need to pay money to subscribe to this internet too. We call this a Usenet provider.Advantages of Usenet:  Downloading from dedicated servers tends to be dramatically faster  Usenet files tend to be consistently better quality compared to torrentsDisadvantages of Usenet:  The cost can be expensive since you need to pay for both a Usenet provider and most Usenet indexers are premium as well  With all the purchasing and setting up of providers, premium indexers, and invite only indexers, Usenet can be a bit overwhelming at first  Uploaded usenet files have an expiration period since they’re on dedicated servers. Media companies can target them more easily with cease and desist lettersI titled this section Usenet vs Torrenting but the truth is it’s not an either or situation. Usenet and torrenting can be setup side by side and really should be. Usenet is best used as a first line of defense, providing your daily downloads and first plan of attack. Torrenting will then be used if the file cannot be found in Usenet – and this can all be automated!We have one more concept to cover, and it’s very related to these topics. The downloaders.Downloading AppsOnce Radarr or Sonarr has used the indexer provided to it by Prowlarr to search and find the best download file, it sends this file link to a downloader app.When it comes to torrenting and usenet, there are a couple popular applications out there. For Usenet, the choice is fairly straight forward in this author’s opinion. I prefer Sabnzbd. Nzbget was a very popular Usenet downloader but since it has been deprecated Sabnzbd has pretty much taken over.Sabnzbd: https://sabnzbd.org/For torrenting there are a lot more viable options. Since I use docker containers for my *ARR stack I chose a container which combines my VPN and a downloader made by someone else. My downloader though is called Transmission. Either way you slice it you’ll need a VPN to accompany your downloader for the torrenting side, and you’ll need to be sure you’re only downloading with it enabled. I happen to use NordVPN.Transmission: https://transmissionbt.com/Transmission/OpenVPN Combo: https://haugene.github.io/docker-transmission-openvpn/Optional appsPlex and the *ARR stack include quite a bit of what I would call optional applications. Here are some and their short descriptions.  Overseerr: You have an automated download stack and you have plex streaming media to your friends. What if instead of bugging you for a new movie they could just trigger the download themselves? Overseerr allows browsing current movies and tv in a slick modern interface and requesting downloads. This request then speaks to your Radarr/Sonarr backend and gets the job done.https://overseerr.dev/  See Ombi as an alternative to Overseerr.https://ombi.io/  Tautulli: Statistics and graphs are cool. Tautulli reaches out to your Plex server and keep statistics on things like who is watching what and how often. The amount of detail Tautulli can pull is very impressive and it displays it in a nice readable web gui.https://tautulli.com/  Bazarr: This is the *ARR stack addition for downloading subtitles.https://wiki.bazarr.media/A Docker compose exampleFor the main portion of my stack I like to run it in Docker compose. Here is my compose file heavily commented to help you.If you need help setting up Docker see my guide here: A deep dive on DockerThis compose file has several things of note:  This creates containers for prowlarr, transmission/openvpn combo, radarr, sonarr, lidarr, sabnzbd, overseerr, and tautulli.  I bind my containers config files to a local config folder with the containers name  I use PUID and GUID 1000 which is the default user.  I mount /bigdaddymnt/data:/mnt/data to my media containers – this is the shared storage where the media is saved which is my NAS.version: \"4.0\"services:  prowlarr:    image: lscr.io/linuxserver/prowlarr:latest    container_name: prowlarr    environment:      - PUID=1000      - PGID=1000      - TZ=America/New_York    volumes:      - ./config/prowlarr:/config    ports:      - 9696:9696    restart: unless-stopped  transmission:        image: haugene/transmission-openvpn        container_name: transmission    volumes:        - /bigdaddymnt/data:/mnt/data # we give it a storage point to download our media to    environment:        - PUID=1000        - PGID=1000        - CREATE_TUN_DEVICE=true        - OPENVPN_PROVIDER=NORDVPN # here you fill in your VPN provider of choice        - NORDVPN_COUNTRY=US # country code (this line is specific to nordvpn)        - OPENVPN_USERNAME=example@gmail.com # your vpn login         - OPENVPN_PASSWORD=REDACTED # your vpn password -- this could be done with environment variable passing for security        - WEBPROXY_ENABLED=false        - TRANSMISSION_DOWNLOAD_DIR=/mnt/data/downloads # download folder within attached volume        - TRANSMISSION_IDLE_SEEDING_LIMIT_ENABLED=true        - TRANSMISSION_SEED_QUEUE_ENABLED=true        - TRANSMISSION_INCOMPLETE_DIR_ENABLED=false        - LOCAL_NETWORK=192.168.10.0/24,10.10.10.3/32,10.10.10.2/32 # These are IPs you want to be allowed to view transmission in browser. Needed due to VPN tunnel.    cap_add:        - NET_ADMIN    logging:        driver: json-file        options:            max-size: 10m    ports:        - \"9091:9091\"    restart: unless-stopped  radarr:    image: linuxserver/radarr    container_name: radarr    hostname: radarr    environment:        - PUID=1000        - PGID=1000        - TZ=America/New_York    volumes:        - ./config/radarr:/config        - /bigdaddymnt/data:/mnt/data    ports:        - 7878:7878    depends_on:        - prowlarr        - transmission    restart: unless-stopped  sonarr:    image: linuxserver/sonarr    container_name: sonarr    environment:        - PUID=1000        - PGID=1000        - TZ=America/New_York    volumes:        - ./config/sonarr:/config        - /bigdaddymnt/data:/mnt/data # our *arr apps need access to the directory transmission is downloading to    ports:        - 8989:8989         depends_on:        - prowlarr        - transmission    restart: unless-stopped    ports:        - 32400:32400    restart: unless-stopped  lidarr:    image: lscr.io/linuxserver/lidarr:latest    container_name: lidarr    environment:      - PUID=1000      - PGID=1000      - TZ=America/New_York    volumes:      - ./config/lidarr:/config      - /bigdaddymnt/data:/mnt/data    ports:      - 8686:8686    restart: unless-stopped  sabnzbd:    image: lscr.io/linuxserver/sabnzbd:latest    container_name: sabnzbd    environment:      - PUID=1000      - PGID=1000      - TZ=America/New_York    volumes:      - ./config/sabnzbd:/config      - /bigdaddymnt/data:/mnt/data      - /plex_temp:/plex_temp    ports:      - 8080:8080    restart: unless-stopped  overseerr:    image: lscr.io/linuxserver/overseerr:latest    container_name: overseerr    environment:      - PUID=1000      - PGID=1000      - TZ=America/New_York    volumes:      - ./config/overseerr:/config    ports:      - 5055:5055    restart: unless-stopped  tautulli:    image: lscr.io/linuxserver/tautulli:latest    container_name: tautulli    environment:      - PUID=1000      - PGID=1000      - TZ=America/New_York    volumes:      - ./config/tautulli:/config    ports:      - 8181:8181    restart: unless-stoppedSummaryIn summary I hope this was helpful in learning a bit about the automated media management ecosystem."
  }
  
]

